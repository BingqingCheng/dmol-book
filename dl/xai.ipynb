{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interpretability in Deep Learning\n",
    "\n",
    "Interpretability, part of the broader topic of explainable AI (XAI), is the process of adding explanations to deep learning model predictions. These explanations should help us understand why particular predictions are made. This is a critical topic because being able to understand model predictions is justified from a practical, theoretical, and increasingly a regulatory stand-point. It is practical because it has been shown that people are more likely to use predictions of a model if they can understand the rationale {cite}`lee2004trust`. Another practical concern is that correctly implementing methods is much easier when one can understand how a model arrived at a prediction. A theoretical justification for transparency is that it can help identify incompleteness in model domains (i.e., covariate shift){cite}`doshi2017towards`. It is now becoming a compliance problem because both the European Union {cite}`goodman2017european` and the G20 {cite}`Development2019` have recently adopted guidelines that recommend or require explanations for machine predictions. The European Union is considering going further with more [strict draft legislation](https://digital-strategy.ec.europa.eu/en/library/proposal-regulation-laying-down-harmonised-rules-artificial-intelligence-artificial-intelligence) being considered. \n",
    "\n",
    "A famous example on the need for explainable AI is found in Caruana et al.{cite}`caruana2015intelligible` who built an ML predictor to assess mortality risk of patients in the ER with pneumonia. The idea is that patients with pneumonia are screened with this tool and it helps doctors know which patients are more at risk of dying. It was found to be quite accurate. When the interpretation of its predictions were examined though, the reasoning was medically insane. The model surprisingly suggested patients with asthma (called asthmatics) have a reduced mortality risk when coming to the ER with pneumonia. Asthma, a condition which makes it difficult to breathe, was found to *make pneumonia patients less likely to die.* This was incidental; asthmatics are actually more at risk of dying from pneumonia but doctors are acutely aware of this and are thus more aggressive and attentive with them. Thanks to the increase care and attention from doctors, there are fewer mortalities. From an empirical standpoint, the model predictions are correct. However if the model were put into practice, it could have cost lives by incorrectly characterizing asthmatics as low mortality risk. Luckily the interpretability of their model helped researchers identify this problem. Thus, we can see that interpretation should always be a step in the construction of predictive models. \n",
    "\n",
    "Deep learning alone is a black box modeling technique. Examining the weights or model equation provides little insight into why predictions are made. Thus, interpretability is an extra task. This is challenge because of both the black box nature of deep learning and because there is no consensus on what exactly constitutes an \"explanation\" for model predictions. {cite}`doshi2017towards`. For some, interpretability means having a natural language explanation justifying a prediction. For others, it can be simply showing which features contributed most to the prediction. \n",
    "\n",
    "There are two broad approaches to interpretation of ML models: post hoc interpretation and self-explaining models {cite}`Murdoch2019`. Self-explaining models are constructed so that an expert can view output of the model and connect it with the features through reasoning. Self-explaining models are highly dependent on the task model{cite}`montavon2018methods`. A familiar example would be a physics based simulation like molecular dynamics or a single-point quantum energy calculation. You can examine the molecular dynamics trajectory, look at output numbers, and an expert can explain why, for example, the simulation predicts a drug molecule will bind to a protein. It may seem like this would be useless for deep learning interpretation. However, we can create a **proxy model** (sometimes **surrogate model**) that is self-explaining and train it to agree with the deep learning model. Why will this training burden be any less than just using the proxy model from the beginning? We can generate an infinite amount of training data because our trained neural network can label arbitrary points. You can also construct deep learning models which have self-explaining features in them, like attention {cite}`bahdanau2014neural`. This allows you to connect the input features to the prediction based on attention. \n",
    "\n",
    "Post hoc interpretation can be approached in a number of ways, but the most common are training data importance, feature importance, and generative explanations{cite}`ribeiro2016should,ribeiro2016model,wachter2017counterfactual`. An example of a post hoc interpretation based on data importance is identifying the most influential training data to explain a prediction {cite}`koh2017understanding`. It is perhaps arguable if this gives an *explanation*, but it certainly helps understand which data is relevant for a prediction. Feature importance are probably the most common XAI approach and frequently appear in computer vision research where the pixels most important for the class of an image are highlighted. Finally, generative explanations are an emerging approach where a new data point is generated (in same distribution as training data features) that serves as a counterfactual. A counterfactual gives insight into how important and sensitive the features are. An example might be in a model that recommends giving a loan. A model could produce the following counterfactual explanation (from {cite}`wachter2017counterfactual`):\n",
    "\n",
    "> You were denied a loan based on your annual income, zip code, and assets. If \n",
    "> your annual income had been $45,000, you would have been offered a loan.\n",
    "\n",
    "The second sentence is the conuterfactual and shows how the features could be changed to affect the model outcome. Coutnerfactuals provide a nice balance of complexity and explanatory power.\n",
    "\n",
    "This was a brief overview of interpretable deep learning. You can find a recent review of interpretable deep learning in Samek et al. {cite}`9369420` and Christopher Molnar has a [broad online book](https://christophm.github.io/interpretable-ml-book/) about interpretable machine learning, including deep learning {cite}`molnar2019`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "remove-cell"
    ]
   },
   "source": [
    "## Feature Importance\n",
    "\n",
    "Feature importance is the most straightforward and common method of interpreting a machine learning model. The output of feature importance is a ranking or numerical values for each feature, typically for a single prediction. If you are trying to understand the feature importance across the whole model, this is called **global** feature importance and **local** for a single prediction. Global feature importance and global interpretability is relatively rare because the best models change which features are important in different regions of feature space.\n",
    "\n",
    "Let's start with a linear model to see feature importance:\n",
    "\n",
    "\\begin{equation}\n",
    "\\hat{y} = \\vec{w}\\vec{x} + b \n",
    "\\end{equation}\n",
    "\n",
    "where $\\vec{x}$ is our feature vector. A simple way to assess feature importance is to simply look at the weight value $w_i$ for a particular feature $x_i$. The weight $w_i$ shows how much $\\hat{y}$ would change if $x_i$ is increased by 1 while all other features are constant. If the magnitude of our features are comparable, then this would be a reasonable way to rank features. However, if our features have units, this approach is sensitive to unit choices and relative magnitude of features. For example if our temperature was changed from Celsius to Fahrenheit, a 1 degree increase will have a smaller effect. \n",
    "\n",
    "To remove the effect of feature magnitude and units, a slightly better way to assess feature importance is to divide $w_i$ by the **standard error**  in the feature values. Recall that standard error is just the ratio of sum of squared error in predicted value divided by the total deviation in the feature. Standard error is a ratio of prediction accuracy to feature variance -- and the prediction accuracy is indepdenent of the feature and thus has no effect for ranking feature importance. $w_i$ divided by standard error is called the $t$-statistic because it can be compared with the $t$-distribution for assessing feature importance.\n",
    "\n",
    "\\begin{equation}\n",
    "t_i = \\frac{w_i}{S_{w_i}},\\; S^2_{w_i} = \\frac{1}{N - D}\\sum_j \\frac{\\left(\\hat{y}_j - y_j\\right)^2}{\\left(x_{ij} - \\bar{x}_i\\right)^2}\n",
    "\\end{equation}\n",
    "\n",
    "where $N$ is the number of examples,  $D$ is the number of features, and $\\bar{x}_i$ is the average value of the $i$th feature. The $t_i$ value can be used to rank features and it can be used for a hypothesis test: if $t_i > 0.05$ then that feature is significant. Note that a feature's significance is sensitive to which features are present in a model; if you add features some may become redundant.\n",
    "\n",
    "If we move to a non-linear learned function $\\hat{f}(\\vec{x})$, we must compute how the prediction changes if a feature value increases by 1 via the derivative approximation:\n",
    "\n",
    "$$\n",
    "\\frac{\\Delta \\hat{f}(\\vec{x})}{\\Delta x_i} \\approx \\frac{\\partial  \\hat{f}(\\vec{x})}{\\partial x_i}\n",
    "$$\n",
    "\n",
    "so a change by 1 is\n",
    "\n",
    "\\begin{equation}\n",
    "\\Delta \\hat{f}(\\vec{x}) \\approx \\frac{\\partial  \\hat{f}(\\vec{x})}{\\partial x_i}.\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "In practice, we make a slight variation on this equation -- instead of a Taylor series centered at 0 approximating this change, we center at some other root (point where the function is 0). This \"grounds\" the series at the decision boundary (a root) and then you can view the partials as \"pushing\" the predicted class away or towards the decision boundary. Another way to think about this is that we use the first-order terms of the Taylor series to build a linear model. Then we just apply what we did above to that linear model and use the coefficients as the \"importance\" of features. Specifically, we use this surrogate function for $\\hat{f}(\\vec{x})$:\n",
    "\n",
    "\\begin{equation}\n",
    "\\require{cancel}\n",
    "\\hat{f}(\\vec{x}) \\approx \\cancelto{0}{f(\\vec{x}')} +  \\nabla\\hat{f}(\\vec{x}')\\cdot\\left(\\vec{x} - \\vec{x}'\\right)\n",
    "\\end{equation}\n",
    "\n",
    "where $\\vec{x}'$ is the root of $\\hat{f}(\\vec{x})$. Note in practice people may choose the trivial root $\\vec{x}' = \\vec{0}$, however a nearby root is ideal. This root is often called the **baseline** input. Note that as opposed to the linear example above, we consider the product of the partial $\\frac{\\partial  \\hat{f}(\\vec{x})}{\\partial x_i}$ and the increase above baseline $(x_i - x_i')$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Network Feature Importance\n",
    "\n",
    "In neural networks the partial derivatives are a poor approximation of the real changes to the output. Small changes to the input can have discontinuous changes, making the terms above have little explanatory power. This is called the **shattered gradients** problem {cite}`pmlr-v70-balduzzi17b`. Breaking down each feature separately also misses correlations between features -- which don't exist in a linear model. Thus the derivative approximation works satisfactorily in locally linear models, but not deep neural networks.\n",
    "\n",
    "There are a variety of techniques that get around the issue of shattered gradients in neural networks. Two popular methods are integrated gradients {cite}`sundararajan2017axiomatic` and SmoothGrad{cite}`smilkov2017smoothgrad`. Integrated gradients creates a path from $\\vec{x}'$ to $\\vec{x}$ and integrates Equation 4 along that path:\n",
    "\n",
    "\\begin{equation}\n",
    "\\textrm{IG}_i = \\left(\\vec{x} - \\vec{x}'\\right) \\int_0^1\\left[\\nabla\\hat{f}\\left(\\vec{x}' + t\\left(\\vec{x} - \\vec{x}'\\right)\\right)\\right]_i\\,dt\n",
    "\\end{equation}\n",
    "\n",
    "where $t$ is some increment along the path such that $\\vec{x}' + t\\left(\\vec{x} - \\vec{x}'\\right) = \\vec{x}'$ when $t = 0$ and $\\vec{x}' + t\\left(\\vec{x} - \\vec{x}'\\right) = \\vec{x}$ when $t = 1$. This gives us the integrated gradient for each feature $i$. The integrated gradients are the importance of each feature, but without the complexity of shattered gradients. There are some nice properties too, like $\\sum_i \\textrm{IG}_i = f(\\vec{x}) - f(\\vec{x}')$ so that the integrated gradients provide a complete partition of the change from the baseline to the prediction{cite}`sundararajan2017axiomatic`.\n",
    "\n",
    "Implementing integrated gradients is actually relatively simple. You approximate the path integral with a Riemann sum by breaking the path into a set of discrete inputs between the input features $\\vec{x}$ and the baseline $\\vec{x}'$. You compute the gradient of these inputs with the neural network. Then you multiply that be the change in features above baseline: $\\left(\\vec{x} - \\vec{x}'\\right)$.\n",
    "\n",
    "SmoothGrad is a similar idea to the integrated gradients. Rather than summing up the gradients along a path though, we sum gradients from random points nearby our prediction. The equation is:\n",
    "\n",
    "\\begin{equation}\n",
    "\\textrm{SG}_i = \\sum_j^M\\left[\\nabla\\hat{f}\\left(\\vec{x}' + \\vec{\\epsilon}\\right)\\right]_i\n",
    "\\end{equation}\n",
    "\n",
    "where $M$ is a choice of sample number and $\\vec{\\epsilon}$ is sampled from $D$ zero-mean Guassians {cite}`smilkov2017smoothgrad`. The only change in implementation here is to replace the path with a series of random perturbations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example\n",
    "\n",
    "Let's see an example of these two methods on a previous model. We'll re-use the {doc}`layer` model for solubility.  As a reminder, the model takes in peptides represented as one-hot amino acid vectors and predicts solubility. The goal of the feature importance method here will be to identify which amino acids matter most for the solubility prediction. The hidden-cell below rebuilds that model, along with a few small changes to make the code/model simpler and more compact. \n",
    "\n",
    "\n",
    "**Work in progress below!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": [
     "hidden-cell"
    ]
   },
   "outputs": [],
   "source": [
    "import haiku as hk\n",
    "import jax\n",
    "import tensorflow as tf\n",
    "import jax.numpy as jnp\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import matplotlib as mpl\n",
    "import warnings\n",
    "import urllib\n",
    "import jax.experimental.optimizers as opt\n",
    "warnings.filterwarnings('ignore')\n",
    "sns.set_context('notebook')\n",
    "sns.set_style('dark',  {'xtick.bottom':True, 'ytick.left':True, 'xtick.color': '#666666', 'ytick.color': '#666666',\n",
    "                        'axes.edgecolor': '#666666', 'axes.linewidth':     0.8 , 'figure.dpi': 300})\n",
    "color_cycle = ['#1BBC9B', '#F06060', '#5C4B51', '#F3B562', '#6e5687']\n",
    "mpl.rcParams['axes.prop_cycle'] = mpl.cycler(color=color_cycle) \n",
    "np.random.seed(0)\n",
    "\n",
    "ALPHABET = ['', 'A','R','N','D','C','Q','E','G','H','I', 'L','K','M','F','P','S','T','W','Y','V']\n",
    "def seq2array(seq):\n",
    "    N = pos_data.shape[-1]\n",
    "    return np.pad(list(map(ALPHABET.index, seq)), (0,N - len(seq))).reshape(1, -1)\n",
    "def array2oh(a):\n",
    "    a = np.squeeze(a)\n",
    "    o = np.zeros((len(a), 21))\n",
    "    o[np.arange(len(a)), a] = 1\n",
    "    return o.astype(np.float32).reshape(1, -1, 21)\n",
    "\n",
    "urllib.request.urlretrieve(\n",
    "    'https://github.com/whitead/dmol-book/raw/master/data/solubility.npz', 'solubility.npz')\n",
    "with np.load('solubility.npz') as r:\n",
    "    pos_data, neg_data = r['positives'], r['negatives']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": [
     "hidden-cell"
    ]
   },
   "outputs": [],
   "source": [
    "# create labels and stich it all into one \n",
    "# tensor\n",
    "labels = np.concatenate((np.ones((pos_data.shape[0], 1), dtype=pos_data.dtype), np.zeros((neg_data.shape[0], 1) , dtype=pos_data.dtype)), axis=0)\n",
    "features = np.concatenate((pos_data, neg_data), axis=0)\n",
    "# we now need to shuffle before creating TF dataset\n",
    "# so that our train/test/val splits are random\n",
    "i = np.arange(len(labels))\n",
    "np.random.shuffle(i)\n",
    "labels = labels[i]\n",
    "features = features[i]\n",
    "L = pos_data.shape[-1]\n",
    "# convert to one-hot\n",
    "ohfeatures = np.zeros((len(labels),L,21))\n",
    "for i in range(len(labels)):\n",
    "    ohfeatures[i,np.arange(L),features[i]] = 1\n",
    "full_data = tf.data.Dataset.from_tensor_slices((ohfeatures.astype(np.float32), labels))\n",
    "\n",
    "# now split into val, test, train\n",
    "N = pos_data.shape[0] + neg_data.shape[0]\n",
    "split = int(0.1 * N)\n",
    "test_data = full_data.take(split).batch(16)\n",
    "nontest = full_data.skip(split)\n",
    "val_data, train_data = nontest.take(split).batch(16), nontest.skip(split).shuffle(1000).batch(16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_cross_entropy(logits, y):\n",
    "    yhat = jax.nn.sigmoid(logits)\n",
    "    return -jnp.sum(y * jnp.log(yhat) + (1 - y) * jnp.log(1 - yhat + 1e-6))\n",
    "def model_fn(x):\n",
    "    # get fractions, excluding skip character\n",
    "    aa_fracs = jnp.mean(x, axis=1)[:,1:]\n",
    "    mask = jnp.sum(x[...,1:], axis=-1, keepdims=True)    \n",
    "    for kernel, pool in zip([5, 3, 3], [4, 2, 2]):\n",
    "        x = hk.Conv1D(16, kernel)(x) * mask\n",
    "        x = jax.nn.tanh(x)\n",
    "        x = hk.MaxPool(pool, pool, 'VALID')(x)\n",
    "        mask = hk.MaxPool(pool, pool, 'VALID')(mask)\n",
    "    x = jnp.concatenate((hk.Flatten()(x), aa_fracs, jnp.sum(x[:,1:], axis=(1,2)).reshape(-1,1)),axis=1)\n",
    "    logits = hk.Sequential([               \n",
    "        hk.Linear(256, with_bias=False), jax.nn.tanh,\n",
    "        hk.Linear(64, with_bias=False), jax.nn.tanh,\n",
    "        hk.Linear(1, with_bias=False)\n",
    "    ])(x)\n",
    "    return logits\n",
    "model = hk.without_apply_rng(hk.transform(model_fn))\n",
    "def loss_fn(params, x, y):    \n",
    "    logits = model.apply(params, x)\n",
    "    return jnp.mean(binary_cross_entropy(logits, y))\n",
    "@jax.jit\n",
    "def solubility_prob(params, x):\n",
    "    logits = model.apply(params, x)\n",
    "    return jax.nn.sigmoid(jnp.squeeze(logits))\n",
    "@jax.jit\n",
    "def accuracy_fn(params, x, y):\n",
    "    logits = model.apply(params, x)\n",
    "    return jnp.mean((logits >= 0) * y + (logits < 0) * (1 - y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = jax.random.PRNGKey(0)\n",
    "xi, yi = ohfeatures[:16], labels[:16]\n",
    "params = model.init(rng, xi)\n",
    "\n",
    "opt_init, opt_update, get_params = opt.adam(1e-3)\n",
    "opt_state = opt_init(params)\n",
    "\n",
    "@jax.jit\n",
    "def update(step, opt_state, x, y):\n",
    "    value, grads = jax.value_and_grad(loss_fn)(get_params(opt_state), x, y)\n",
    "    opt_state = opt_update(step, grads, opt_state)\n",
    "    return value, opt_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12.355796\n",
      "8.53301\n",
      "7.908889\n",
      "8.962775\n",
      "11.328706\n",
      "3.9574752\n",
      "8.746103\n",
      "3.6040251\n",
      "6.791431\n",
      "5.3469744\n",
      "3.8407235\n",
      "8.486663\n",
      "4.9943523\n",
      "3.8833294\n",
      "6.215328\n",
      "4.0143185\n",
      "4.365747\n",
      "2.039011\n",
      "2.888486\n",
      "3.7547698\n",
      "2.507792\n",
      "6.383517\n",
      "3.2257693\n",
      "4.652371\n",
      "7.1247816\n",
      "1.3861761\n",
      "2.753914\n",
      "15.291146\n",
      "6.1099734\n",
      "0.48300183\n",
      "1.9837027\n",
      "1.1322528\n"
     ]
    }
   ],
   "source": [
    "losses = []\n",
    "epochs = 32\n",
    "for e in range(epochs):\n",
    "    for i, (xi, yi) in enumerate(train_data):\n",
    "        value, opt_state = update(i, opt_state, xi.numpy(), yi.numpy())\n",
    "        losses.append(value)\n",
    "    print(losses[-2])\n",
    "opt_params = get_params(opt_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.57661635\n"
     ]
    }
   ],
   "source": [
    "acc = []\n",
    "for xi, yi in test_data:\n",
    "    acc.append(accuracy_fn(opt_params, xi.numpy(), yi.numpy()))\n",
    "print(jnp.mean(np.array(acc)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try an amino acid sequence, a peptide, to get a feel for the model. The model outputs logits (logarithm of odds), which we put through a sigmoid to get probabilities. The peptides must be converted from a sequence to a matrix of one-hot column vectors. We'll try two sequences: multiple serines which should be soluble and multiple phenylalanines, which should be insoluble. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probability SSS of being soluble 0.68\n",
      "Probability VVV of being soluble 0.35\n"
     ]
    }
   ],
   "source": [
    "s = 'SSS'\n",
    "sm = array2oh(seq2array(s))\n",
    "sol_prob = solubility_prob(opt_params, sm)\n",
    "print(f'Probability {s} of being soluble {sol_prob:.2f}')\n",
    "\n",
    "s = 'VVV'\n",
    "sm = array2oh(seq2array(s))\n",
    "sol_prob = solubility_prob(opt_params, sm)\n",
    "print(f'Probability {s} of being soluble {sol_prob:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks reasonable -- the hydrophilic are soluble and the hydrophobic is insoluble. We'll begin by computing the gradients with respect to input -- the naieve approach that is susceptible to shattered gradients. Computing this is a component in the process for integrated and smooth gradients, so not wasted effort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "def plot_grad(g, s):\n",
    "    #g = np.array(g)\n",
    "    h = g[0, np.arange(len(s)), list(map(ALPHABET.index,s))]\n",
    "    plt.bar(np.arange(len(s)), height=h)\n",
    "    plt.gca().set_xticks(range(len(s)))\n",
    "    plt.gca().set_xticklabels(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probability EKAALSLVVVFFRPVC of being soluble 0.19\n"
     ]
    }
   ],
   "source": [
    "s = 'EKAALSLVVVFFRPVC'\n",
    "sm = array2oh(seq2array(s))\n",
    "sol_prob = solubility_prob(opt_params, sm)\n",
    "print(f'Probability {s} of being soluble {sol_prob:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYEAAAD7CAYAAACMlyg3AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8vihELAAAACXBIWXMAAAsTAAALEwEAmpwYAAAWPUlEQVR4nO3dfWxT1/3H8U/shAaHPJmsaRsC2SgFOkZC0wWHhYcoWsVgYepWARorGxIaWifUFbExStoxJUho8tqqmVaxVYJOomLakCoIaksXTaiQhZBQQAMNxtQR6BApsRMyW/xIsX9/ILIah4f4nsQO5/2SkOzr4+85mPp+es+59zotGo1GBQCwkivZAwAAJA8hAAAWIwQAwGKEAABYjBAAAIulJ3sA9+ratWs6d+6ccnNz5XKRXQBwLyKRiHp7ezVp0iSNGTMm7vVREwLnzp2T3+9P9jAAYFRav369pkyZErd91IRATk6OJGn16ueUm5uX3MEAwCjR29ujN9/87cA+9FajJgTcbrckKTc3T/n53iSPBgBGl5v70FsxuQ4AFjN2JPDnP/9ZR48eVXd3t15++WUVFRXFtYlEItq1a5dOnjyptLQ0LVy4UFVVVaaGAAAYImNHAmVlZVq/fr3Gjx9/2zaHDx/Wp59+qvr6em3YsEF79+7V5cuXTQ0BADBExkLg0Ucfldd757n69vZ2VVVVyeVyKTs7W2VlZero6DA1BADAEI3ownAgEIg5UvB6vQoGg3HtwuGwwuFwzLbB2gEAnEnJs4Oam5vV1NSU7GEAwH1vREPA6/Wqu7tbJSUlkm4cGQw2hVRTU6PKysqYbcFgkIvFAMCwEQ2B8vJyHTx4ULNmzVIoFNKxY8f005/+NK6dx+ORx+MZyaElVVZepjwZGQm9N9zfr1DPVcMjAmALYyGwa9cuffTRR7py5Ypee+01ZWVlafPmzWpsbFRtba1KSkrk8/n08ccf66WXXpIkLV68WAUFBaaGMGp5MjI0ofnNhN57oWa1QiIEACTGWAgsX75cy5cvj9u+du3agccul0srVqww1SUAwCGuGAYAixECAGAxQgAALEYIAIDFCAEAsBghAAAWIwQAwGKEAABYjBAAAIsRAgBgMUIAACxGCACAxQgBALAYIQAAFiMEAMBihAAAWIwQAACLEQIAYDFCAAAsZuw3hoG7ycrLlCcjI6H3hvv7Feq5anhEAAiB+4yTHa00vDtbT0aGJjS/mdB7L9SsVkiEAGAaIXCfcbKjldjZArZhTQAALMaRAGCJVJ4qRPIYC4FLly5p+/btCoVCysrK0qpVq1RYWBjTZu/evTpw4IByc3MlSZMnT9Z3v/tdU0MAcAdMFWIwxkJg586dWrBggXw+n1pbW7Vz506tW7curp3P59MzzzxjqlsAgANG1gSuXLmizs5OVVRUSJIqKirU2dmpvr4+E+UBAMPEyJFAMBhUXl6eXK4bmeJyuZSXl6dgMKjs7OyYtkeOHNGpU6eUk5Oj2tpaTZ48Oa5eOBxWOByO6wMAYNaILgzPmzdPixYtktvt1qlTp/TGG29o8+bNGjduXEy75uZmNTU1jeTQMMqwyAmYYSQE8vPz1dPTo0gkIpfLpUgkop6eHuXn58e0u7kgLEmPP/648vPz9Z///EePPfZYTLuamhpVVlbGbAsGg/L7/SaGi/sAi5yAGUZCICcnR8XFxWpra5PP51NbW5uKi4vjpoKCweBAMJw/f17d3d1xZxBJksfjkcfjMTE0AMAdGJsOWrFihbZv3659+/bJ4/Fo1apVkqTGxkbV1taqpKRE77zzjjo7O+VyueR2u7Vq1aqYowMAowPTcfcPYyHw0EMPaePGjXHb165dO/D4ZjAAGN2Yjrt/cNsIALAYIQAAFiMEAMBihAAAWIwQAACLEQIAYDFCAAAsRggAgMUIAQCwGCEAABYjBADAYoQAAFiMEAAAixECAGAxQgAALDaivzEMpCJ+IAU2IwRgPX4gBTZjOggALEYIAIDFCAEAsBhrArgjJ4umLJgCqY8QwB05WTRlwRRIfUwHAYDFCAEAsJix6aBLly5p+/btCoVCysrK0qpVq1RYWBjTJhKJaNeuXTp58qTS0tK0cOFCVVVVmRoCcN9hTQbDzVgI7Ny5UwsWLJDP51Nra6t27typdevWxbQ5fPiwPv30U9XX1ysUCqmhoUHTpk1TQUGBqWEA9xXWZDDcjEwHXblyRZ2dnaqoqJAkVVRUqLOzU319fTHt2tvbVVVVJZfLpezsbJWVlamjo8PEEAAACTByJBAMBpWXlyeX60amuFwu5eXlKRgMKjs7e6BdIBDQ+PHjB557vV4Fg8G4euFwWOFwOK4PAIBZKXmKaHNzs5qamozXNTm/arJWuL9fF2pWJ1zr1ueJ1jJdL1Vr3VrP9Gdmw38bpj8zj2eMsrIeSKhWKPR/CoevDTxPle/5rfVS9UaFRkIgPz9fPT09ikQicrlcikQi6unpUX5+fkw7r9er7u5ulZSUSLpxZOD1euPq1dTUqLKyMmZbMBiU3+93NE6T86sma4V6rhqbuzVZy3Q9G2pJdvy3YfozC4evxezInUiV7/mt9VL1RoVG1gRycnJUXFystrY2SVJbW5uKi4tjpoIkqby8XAcPHlQkElFfX5+OHTum8vLyuHoej0cFBQUxf24NFACAc8amg1asWKHt27dr37598ng8WrVqlSSpsbFRtbW1Kikpkc/n08cff6yXXnpJkrR48WLODAKAJDIWAg899JA2btwYt33t2rUDj10ul1asWGGqSwCAQ1wxDAAWS8mzg4aLyTMtAOB+YFUImD6jAQBGO6aDAMBihAAAWIwQAACLEQIAYDFCAAAsRggAgMUIAQCwGCEAABYjBADAYoQAAFjMqttGALj/cY+woSEEANxXuEfY0BACAHAbpn9LORURAgBwGzYcVbAwDAAWIwQAwGKEAABYjBAAAIsRAgBgMUIAACxGCACAxRxfJ3Dt2jXt2LFDnZ2dcrlceuaZZzRz5sy4dqdPn1ZjY6MKCwtvdJyero0bNzrtHgDggOMQ2L9/v8aOHauGhgZdunRJfr9f9fX1yszMjGv78MMPa9OmTU67BAAY4ng6qL29XXPnzpUkFRYWatKkSTp58qTjgQEAhp/jI4FAIKDx48cPPPd6vQoEAoO27erqUkNDg9xutxYsWKDKyspB24XDYYXD4ZhtwWDQ6VABALe4awg0NDTcdqfu9/vvuaOJEydq69atGjt2rC5fvqxXX31VeXl5mj59elzb5uZmNTU13XNtAEBi7hoCdXV1d3zd6/Wqu7tb2dnZkm4cGUydOjWu3dixYwceFxQUqKysTGfPnh00BGpqauKOEoLB4JBCBwBwd47XBMrLy/Xhhx9Kki5duqR///vf+vKXvxzXrre3V9FoVJIUCoV06tQpFRcXD1rT4/GooKAg5k9+fr7ToQIAbuF4TeCpp57Sjh07VFdXJ5fLpe9973sDZwbt2bNHubm5mj9/vo4ePaoDBw7I7XYrEonI5/OprKzMafcAAAcch8ADDzygNWvWDPrakiVLBh5XV1erurraaXcAAIO4YhgALEYIAIDFCAEAsBghAAAWIwQAwGKEAABYjBAAAIsRAgBgMUIAACxGCACAxQgBALAYIQAAFnN8Azlbhfv7daFmdcLvBYBUQAgkKNRzVSFdTfYwAMARpoMAwGKEAABYjBAAAIsRAgBgMUIAACxGCACAxQgBALAYIQAAFuNiMQAYAU7uMnDz/cOBEACAEZCqdxlwHAKtra3av3+/Ll68qKVLl6q6uvq2bT/88EO9//77ikajmjFjhpYtWyaXixkpAEgWx3vg4uJirV69WhUVFXdsd/nyZTU1NWnDhg2qr69XV1eXDh8+7LR7AIADjo8EioqKJElpaWl3bNfR0aGysjJlZ2dLkqqqqtTS0qLKysq4tuFwWOFwOGZbMBh0OlQAwC1GbE0gEAjI6/UOPPd6vbfdsTc3N6upqWmkhgYA1rprCDQ0NCgQCAz6mt/vH5Y5/ZqamrgjhGAwKL/fb7wvALDZXUOgrq7OSEderzcmTAKBgPLz8wdt6/F45PF4jPQLALi9ETs154knntCxY8fU19enSCSigwcP6sknnxyp7gEAg3C8JtDW1qbdu3crHA7r+PHjeu+99/T888/rkUce0Z49e5Sbm6v58+frC1/4ghYvXqytW7dKkh5//HHNnj3b8V8AAJA4xyFQUVFx29NDlyxZEvN83rx5mjdvntMuAQCGcKUWAFiMEAAAixECAGAxQgAALEYIAIDFCAEAsBghAAAWIwQAwGKEAABYjBAAAIsRAgBgMUIAACxGCACAxQgBALAYIQAAFiMEAMBihAAAWIwQAACLEQIAYDFCAAAsRggAgMUIAQCwGCEAABYjBADAYulOC7S2tmr//v26ePGili5dqurq6kHbnT59Wo2NjSosLLzRcXq6Nm7c6LR7AIADjkOguLhYq1ev1vvvv3/Xtg8//LA2bdrktEsAgCGOQ6CoqEiSlJaW5ngwN4XDYYXD4ZhtwWDQWH0AwA2OQ2Aourq61NDQILfbrQULFqiysnLQds3NzWpqahrJoQGAle4aAg0NDQoEAoO+5vf75XLd29ryxIkTtXXrVo0dO1aXL1/Wq6++qry8PE2fPj2ubU1NTVxABINB+f3+e+oLAHBv7hoCdXV1RjoaO3bswOOCggKVlZXp7Nmzg4aAx+ORx+Mx0i8A4PZG7BTR3t5eRaNRSVIoFNKpU6dUXFw8Ut0DAAbheE2gra1Nu3fvVjgc1vHjx/Xee+/p+eef1yOPPKI9e/YoNzdX8+fP19GjR3XgwAG53W5FIhH5fD6VlZUZ+CsAABLlOAQqKipUUVEx6GtLliwZeFxdXX3bawgAAMnBFcMAYDFCAAAsRggAgMUIAQCwGCEAABYjBADAYoQAAFiMEAAAixECAGAxQgAALEYIAIDFCAEAsBghAAAWIwQAwGKEAABYjBAAAIsRAgBgMUIAACxGCACAxQgBALAYIQAAFiMEAMBihAAAWIwQAACLpTst8Pbbb+sf//iHMjIy9MADD2jp0qUqKSkZtO2+ffvU0tIiSZozZ44WL17stHsAgAOOQ2DGjBlatmyZ3G63Tpw4od///vfasmVLXLszZ86oo6NDv/jFLyRJW7du1ZQpU/TYY485HQIAIEGOQ2DmzJkDj7/0pS+pp6dHkUhELlfsTFN7e7t8Pp/GjBkjSfL5fGpvbx80BMLhsMLhcMy2YDDodKgAgFs4DoHP++tf/6oZM2bEBYAkBQIBTZ06deC51+vVP//5z0HrNDc3q6mpyeTQAACDuGsINDQ0KBAIDPqa3+8f2OEfOXJER44c0fr16x0PqqamRpWVlTHbgsGg/H6/49oAgP+5awjU1dXdtchHH32kd955Ry+88IJycnIGbeP1etXd3T3wPBAIKD8/f9C2Ho9HHo/nrv0CqSjc368LNasTfi8wkhxPB504cUJ/+tOf9JOf/EQFBQW3bVdeXq4//vGPWrBggSSptbVVy5cvd9o9kHJCPVcV0tVkDwO4J45D4K233lJ6erq2bds2sO2FF17QuHHj9Ic//EGlpaUqLS3V1KlTNWvWLP3yl7+UdGNhmDODACC5HIfAr3/969u+tnLlypjntbW1qq2tddolAMAQrhgGAIsRAgBgMUIAACxGCACAxQgBALCY0dtGDKfr169Lknp7e5I7EAAYRW7uM2/uQ281akLgypUrkqQ33/xtkkcCAKPPlStXVFhYGLc9LRqNRpMwniG7du2azp07p9zc3EFvUOfEzfsSrV+//ra3skhWPRtqpfLYUrVWKo8tVWul8thM/z0/LxKJqLe3V5MmTRq4i/PnjZojgTFjxmjKlCnD2kd+fv4db32RzHo21DJdz4ZapuvZUMt0vVSt9XkPPvjgbV9jYRgALEYIAIDFCAEAsBghoBu/X/DNb37T2G8YmKxnQy3T9WyoZbqeDbVM10vVWkM1as4OAgCYx5EAAFiMEAAAi42a6wSGy4svvqj09HRlZGQMbPvRj36U0Lm6L774on784x+rqKhI165d0xtvvKHc3FytXLkyoQvcQqGQNmzYoLlz52rZsmVDfv9w1fr839OEjo4Ovfvuu4pGo+rv79fEiRO1enViv9FrYmyvv/66SktLNX/+/IFt0WhUdXV1+v73vz/kX8QzWc/02KT478DUqVO1dOnSIdcZzlqfffaZvv71r6uqqspxrUgkokWLFumrX/3qkOsMx+d//fp17du3T0eOHFFGRoZcLpemTp2qb3/723K73UOuN1TWh4AkrVmzxtgOTZLC4bB+85vfaNKkSVq6dKnS0tISqtPW1qYvfvGLOnLkiL7zne8oPT3xfy6TtUzq7e3V22+/rU2bNsnr9Soajer8+fNJHdPXvvY1ffDBBzFf9DNnzigtLS2hCxZN1jM9tptMfgeGo9Ynn3yiLVu2aMaMGcrLy3NUq7OzU7/61a80ffp0jRs3bkg1huPz37Fjh/r7+7Vp0yZlZmbq+vXrOnTokPr7+0ckBJgOMqyvr0+vvPKKpk2bpmXLliUcAJLU0tKiRYsWqaioSMePH3c0LpO1TOrt7ZXb7R74MqalpWnixIlJHVNpaam6urp08eLFgW0tLS2aM2dOQv+eJuuZHttoUVRUJI/Ho56eHse1Jk6cqMzMTF2+fHnI7zX9+V+6dEnHjh3Ts88+q8zMTEmS2+3WvHnzBp4PN0JA0rZt21RfX6/6+npt2bLFUa3f/e53mjlzppYsWeKozoULFxQKhTRt2jTNmTNHhw4dSolapk2YMEElJSX6+c9/rm3btukvf/mL/vvf/yZ1TOnp6Zo9e7ZaWlokSVevXtWxY8dUWVmZ9Hqmx3bT578DJ0+eTJlaN509e1bjxo3ThAkTHNc6ffq0+vv773grhdsx/fmfP39eDz74oLKyshJ6vwmpMSeQZCYPX7/yla+ovb1d8+bNS/iwVZIOHTokn8+ntLQ0zZo1S7t27VIwGEzo5lIma5nmcrn03HPP6ZNPPtGZM2d0/PhxffDBB3r55ZeT+sWYM2eOXn/9dT399NNqb2/X5MmTHX1eJuuZHpuUutNB27ZtkyR1dXXphz/8oaNpzG3btikjI0OZmZlas2ZNwufkD8fnn0yEgGFPPfWUTpw4oVdeeUXr1q1LKAg+++wztbW1KT09Xa2trZJuLB797W9/06JFi5JWazgVFRWpqKhI1dXV2rx5s06fPq0nnngiaeMpLi5WXl6e/v73v6ulpUU1NTUpU8/02FLZzUDp6OjQW2+9pUcffVQ5OTmOajll+t+yq6tLoVAoaf/TQwgMg2984xuKRqMJB8Hx48dVWFion/3sZwPb/vWvf2nHjh1D3nGbrDUcgsGgAoGAJk+ePPC8r69vWO6kOFRz5szR3r17FQgEVFpamlL1TI8t1ZWXl6u9vV3vvvuu47PbTDD1+RcWFqq0tFQ7d+7UypUrlZmZqUgkopaWFj355JMjsi5ACOh/h4k3PfvssyopKXFU8+YONpEgOHTokGbPnh2zbfLkyYpEIjpz5syQTkMzWevzXnvttZjTXhOdvolEIgNfpoyMDEWjUX3rW99ytDhsamwVFRXavXu35s6da+RsKpP1TI9tNHj66ae1ZcsWLVy4ULm5uUkdi8nP/wc/+IGampq0ZcsWpaenKxqNasaMGTH7pOHEbSMAwGKcHQQAFiMEAMBihAAAWIwQAACLEQIAYDFCAAAsRggAgMUIAQCw2P8Dh1YYExT277kAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "g = jax.grad(solubility_prob,1)(opt_params, sm)\n",
    "plot_grad(g, s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def integrated_gradients(sm, N):\n",
    "    baseline = jnp.zeros((1, L, 21))\n",
    "    t = jnp.linspace(0, 1, N).reshape(-1, 1, 1)\n",
    "    path = baseline * (1 - t) + sm * t\n",
    "    def get_grad(pi):\n",
    "        # compute gradient \n",
    "        # add/remove batch axes\n",
    "        return jax.grad(solubility_prob, 1)(opt_params, pi[jnp.newaxis, ...])[0]\n",
    "    gs = jax.vmap(get_grad)(path)\n",
    "    # sum pieces (Riemann sum), multiply by (x - x')\n",
    "    ig = jnp.mean(gs, axis=0, keepdims=True) * (sm - baseline)\n",
    "    return ig\n",
    "ig = integrated_gradients(sm, 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_grad(ig, s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def smooth_gradients(sm, N, rng, sigma=1e-3):\n",
    "    baseline = jnp.zeros((1, L, 21))\n",
    "    t = jax.random.normal(rng, shape=(N, sm.shape[1], sm.shape[2])) * sigma\n",
    "    path = sm + t\n",
    "    # remove examples that are negative and force summing to 1\n",
    "    path = jnp.clip(path, 0, 1)\n",
    "    path /= jnp.sum(path, axis=2, keepdims=True)    \n",
    "    def get_grad(pi):\n",
    "        # compute gradient \n",
    "        # add/remove batch axes\n",
    "        return jax.grad(solubility_prob, 1)(opt_params, pi[jnp.newaxis, ...])[0]\n",
    "    gs = jax.vmap(get_grad)(path)\n",
    "    # mean\n",
    "    ig = jnp.mean(gs, axis=0, keepdims=True)\n",
    "    return ig\n",
    "sg = smooth_gradients(sm, 256, jax.random.PRNGKey(0))\n",
    "plot_grad(sg, s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cited References\n",
    "\n",
    "```{bibliography}\n",
    ":style: unsrtalpha\n",
    ":filter: docname in docnames\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
