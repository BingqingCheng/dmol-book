{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interpretability in Deep Learning\n",
    "\n",
    "```{warning}\n",
    "This chapter is being written.\n",
    "```\n",
    "\n",
    "Interpretability, part of the broader topic of explainable AI (XAI), is the process of adding explanations to deep learning model predictions. These explanations should help us understand why particular predictions are made. This is a critical topic because being able to understand model predictions is justified from a practical, theoretical, and increasingly a regulatory stand-point. It is practical because it has been shown that people are more likely to use predictions of a model if they can understand the rationale {cite}`lee2004trust`. Another practical concern is that correctly implementing methods is much easier when one can understand how a model arrived at a prediction. A theoretical justification for transparency is that it can help identify incompleteness in model domains (i.e., covariate shift){cite}`doshi2017towards`. It is now becoming a compliance problem because both the European Union {cite}`goodman2017european` and the G20 {cite}`Development2019` have recently adopted guidelines that recommend or require explanations for machine predictions. The European Union is considering going further with more [strict draft legislation](https://digital-strategy.ec.europa.eu/en/library/proposal-regulation-laying-down-harmonised-rules-artificial-intelligence-artificial-intelligence) being considered. \n",
    "\n",
    "A famous example on the need for explainable AI is found in Caruana et al.{cite}`caruana2015intelligible` who built an ML predictor to assess mortality risk of patients in the ER with pneumonia. The idea is that patients with pneumonia are screened with this tool and it helps doctors know which patients are more at risk of dying. It was found to be quite accurate. When the interpretation of its predictions were examined though, the reasoning was medically insane. The model surprisingly suggested patients with asthma (called asthmatics) have a reduced mortality risk when coming to the ER with pneumonia. Asthma, a condition which makes it difficult to breathe, was found to *make pneumonia patients less likely to die.* This was incidental; asthmatics are actually more at risk of dying from pneumonia but doctors are acutely aware of this and are thus more aggressive and attentive with them. Thanks to the increase care and attention from doctors, there are fewer mortalities. From an empirical standpoint, the model predictions are correct. However if the model were put into practice, it could have cost lives by incorrectly characterizing asthmatics as low mortality risk. Luckily the interpretability of their model helped researchers identify this problem. Thus, we can see that interpretation should always be a step in the construction of predictive models. \n",
    "\n",
    "Deep learning alone is a black box modeling technique. Examining the weights or model equation provides little insight into why predictions are made. Thus, interpretability is an extra task. This is challenge because of both the black box nature of deep learning and because there is no consensus on what exactly constitutes an \"explanation\" for model predictions. {cite}`doshi2017towards`. For some, interpretability means having a natural language explanation justifying a prediction. For others, it can be simply showing which features contributed most to the prediction. \n",
    "\n",
    "There are two broad approaches to interpretation of ML models: post hoc interpretation and self-explaining models {cite}`Murdoch2019`. Self-explaining models are constructed so that an expert can view output of the model and connect it with the features through reasoning. Self-explaining models are highly dependent on the task model{cite}`montavon2018methods`. A familiar example would be a physics based simulation like molecular dynamics or a single-point quantum energy calculation. You can examine the molecular dynamics trajectory, look at output numbers, and an expert can explain why, for example, the simulation predicts a drug molecule will bind to a protein. It may seem like this would be useless for deep learning interpretation. However, we can create a **proxy model** (sometimes **surrogate model**) that is self-explaining and train it to agree with the deep learning model. Why will this training burden be any less than just using the proxy model from the beginning? We can generate an infinite amount of training data because our trained neural network can label arbitrary points. You can also construct deep learning models which have self-explaining features in them, like attention {cite}`bahdanau2014neural`. This allows you to connect the input features to the prediction based on attention. \n",
    "\n",
    "Post hoc interpretation can be approached in a number of ways, but the most common are training data importance, feature importance, and generative explanations{cite}`ribeiro2016should,ribeiro2016model,wachter2017counterfactual`. An example of a post hoc interpretation based on data importance is identifying the most influential training data to explain a prediction {cite}`koh2017understanding`. It is perhaps arguable if this gives an *explanation*, but it certainly helps understand which data is relevant for a prediction. Feature importance are probably the most common XAI approach and frequently appear in computer vision research where the pixels most important for the class of an image are highlighted. Finally, generative explanations are an emerging approach where a new data point is generated (in same distribution as training data features) that serves as a counterfactual. A counterfactual gives insight into how important and sensitive the features are. An example might be in a model that recommends giving a loan. A model could produce the following counterfactual explanation (from {cite}`wachter2017counterfactual`):\n",
    "\n",
    "> You were denied a loan based on your annual income, zip code, and assets. If \n",
    "> your annual income had been $45,000, you would have been offered a loan.\n",
    "\n",
    "The second sentence is the conuterfactual and shows how the features could be changed to affect the model outcome. Coutnerfactuals provide a nice balance of complexity and explanatory power.\n",
    "\n",
    "This was a brief overview of interpretable deep learning. You can find a recent review of interpretable deep learning in Samek et al. {cite}`9369420` and Christopher Molnar has a [broad online book](https://christophm.github.io/interpretable-ml-book/) about interpretable machine learning, including deep learning {cite}`molnar2019`. Prediction error and confidence in predictions is not covered here, but see the methods from {doc}`../ml/regression` which apply. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Importance\n",
    "\n",
    "Feature importance is the most straightforward and common method of interpreting a machine learning model. The output of feature importance is a ranking or numerical values for each feature, typically for a single prediction. If you are trying to understand the feature importance across the whole model, this is called **global** feature importance and **local** for a single prediction. Global feature importance and global interpretability is relatively rare because the best models change which features are important in different regions of feature space.\n",
    "\n",
    "Let's start with a linear model to see feature importance:\n",
    "\n",
    "\\begin{equation}\n",
    "\\hat{y} = \\vec{w}\\vec{x} + b \n",
    "\\end{equation}\n",
    "\n",
    "where $\\vec{x}$ is our feature vector. A simple way to assess feature importance is to simply look at the weight value $w_i$ for a particular feature $x_i$. The weight $w_i$ shows how much $\\hat{y}$ would change if $x_i$ is increased by 1 while all other features are constant. If the magnitude of our features are comparable, then this would be a reasonable way to rank features. However, if our features have units, this approach is sensitive to unit choices and relative magnitude of features. For example if our temperature was changed from Celsius to Fahrenheit, a 1 degree increase will have a smaller effect. \n",
    "\n",
    "To remove the effect of feature magnitude and units, a slightly better way to assess feature importance is to divide $w_i$ by the **standard error**  in the feature values. Recall that standard error is just the ratio of sum of squared error in predicted value divided by the total deviation in the feature. Standard error is a ratio of prediction accuracy to feature variance -- and the prediction accuracy is indepdenent of the feature and thus has no effect for ranking feature importance. $w_i$ divided by standard error is called the $t$-statistic because it can be compared with the $t$-distribution for assessing feature importance.\n",
    "\n",
    "\\begin{equation}\n",
    "t_i = \\frac{w_i}{S_{w_i}},\\; S^2_{w_i} = \\frac{1}{N - D}\\sum_j \\frac{\\left(\\hat{y}_j - y_j\\right)^2}{\\left(x_{ij} - \\bar{x}_i\\right)^2}\n",
    "\\end{equation}\n",
    "\n",
    "where $N$ is the number of examples,  $D$ is the number of features, and $\\bar{x}_i$ is the average value of the $i$th feature. The $t_i$ value can be used to rank features and it can be used for a hypothesis test: if $t_i > 0.05$ then that feature is significant. Note that a feature's significance is sensitive to which features are present in a model; if you add features some may become redundant.\n",
    "\n",
    "If we move to a non-linear learned function $\\hat{f}(\\vec{x})$, we must compute how the prediction changes if a feature value increases by 1 via the derivative approximation:\n",
    "\n",
    "$$\n",
    "\\frac{\\Delta \\hat{f}(\\vec{x})}{\\Delta x_i} \\approx \\frac{\\partial  \\hat{f}(\\vec{x})}{\\partial x_i}\n",
    "$$\n",
    "\n",
    "so a change by 1 is\n",
    "\n",
    "\\begin{equation}\n",
    "\\Delta \\hat{f}(\\vec{x}) \\approx \\frac{\\partial  \\hat{f}(\\vec{x})}{\\partial x_i}.\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "In practice, we make a slight variation on this equation -- instead of a Taylor series centered at 0 approximating this change, we center at some other root (point where the function is 0). This \"grounds\" the series at the decision boundary (a root) and then you can view the partials as \"pushing\" the predicted class away or towards the decision boundary. Another way to think about this is that we use the first-order terms of the Taylor series to build a linear model. Then we just apply what we did above to that linear model and use the coefficients as the \"importance\" of features. Specifically, we use this surrogate function for $\\hat{f}(\\vec{x})$:\n",
    "\n",
    "\\begin{equation}\n",
    "\\require{cancel}\n",
    "\\hat{f}(\\vec{x}) \\approx \\cancelto{0}{f(\\vec{x}')} +  \\nabla\\hat{f}(\\vec{x}')\\cdot\\left(\\vec{x} - \\vec{x}'\\right)\n",
    "\\end{equation}\n",
    "\n",
    "where $\\vec{x}'$ is the root of $\\hat{f}(\\vec{x})$. Note in practice people may choose the trivial root $\\vec{x}' = \\vec{0}$, however a nearby root is ideal. This root is often called the **baseline** input. Note that as opposed to the linear example above, we consider the product of the partial $\\frac{\\partial  \\hat{f}(\\vec{x})}{\\partial x_i}$ and the increase above baseline $(x_i - x_i')$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Network Feature Importance\n",
    "\n",
    "In neural networks the partial derivatives are a poor approximation of the real changes to the output. Small changes to the input can have discontinuous changes, making the terms above have little explanatory power. This is called the **shattered gradients** problem {cite}`pmlr-v70-balduzzi17b`. Breaking down each feature separately also misses correlations between features -- which don't exist in a linear model. Thus the derivative approximation works satisfactorily in locally linear models, but not deep neural networks.\n",
    "\n",
    "There are a variety of techniques that get around the issue of shattered gradients in neural networks. Two popular methods are integrated gradients {cite}`sundararajan2017axiomatic` and SmoothGrad{cite}`smilkov2017smoothgrad`. Integrated gradients creates a path from $\\vec{x}'$ to $\\vec{x}$ and integrates Equation 4 along that path:\n",
    "\n",
    "\\begin{equation}\n",
    "\\textrm{IG}_i = \\left(\\vec{x} - \\vec{x}'\\right) \\int_0^1\\left[\\nabla\\hat{f}\\left(\\vec{x}' + t\\left(\\vec{x} - \\vec{x}'\\right)\\right)\\right]_i\\,dt\n",
    "\\end{equation}\n",
    "\n",
    "where $t$ is some increment along the path such that $\\vec{x}' + t\\left(\\vec{x} - \\vec{x}'\\right) = \\vec{x}'$ when $t = 0$ and $\\vec{x}' + t\\left(\\vec{x} - \\vec{x}'\\right) = \\vec{x}$ when $t = 1$. This gives us the integrated gradient for each feature $i$. The integrated gradients are the importance of each feature, but without the complexity of shattered gradients. There are some nice properties too, like $\\sum_i \\textrm{IG}_i = f(\\vec{x}) - f(\\vec{x}')$ so that the integrated gradients provide a complete partition of the change from the baseline to the prediction{cite}`sundararajan2017axiomatic`.\n",
    "\n",
    "Implementing integrated gradients is actually relatively simple. You approximate the path integral with a Riemann sum by breaking the path into a set of discrete inputs between the input features $\\vec{x}$ and the baseline $\\vec{x}'$. You compute the gradient of these inputs with the neural network. Then you multiply that be the change in features above baseline: $\\left(\\vec{x} - \\vec{x}'\\right)$.\n",
    "\n",
    "SmoothGrad is a similar idea to the integrated gradients. Rather than summing up the gradients along a path though, we sum gradients from random points nearby our prediction. The equation is:\n",
    "\n",
    "\\begin{equation}\n",
    "\\textrm{SG}_i = \\sum_j^M\\left[\\nabla\\hat{f}\\left(\\vec{x}' + \\vec{\\epsilon}\\right)\\right]_i\n",
    "\\end{equation}\n",
    "\n",
    "where $M$ is a choice of sample number and $\\vec{\\epsilon}$ is sampled from $D$ zero-mean Guassians {cite}`smilkov2017smoothgrad`. The only change in implementation here is to replace the path with a series of random perturbations.\n",
    "\n",
    "Beyond these gradient based approaches, Layer-wise Relevance Propagation (LRP) is another popular approach for feature importance analysis in neural networks. LRP works by doing a backwards propogation through the neural network that partitions the output value of one layer to the input features. It \"distributes relevance\". What is unusual about LRP is that each layer type needs its own implementation. It doesn't rely on the analytic derivative, but instead a Taylor series expansion of the layer equation. There are variants for GNNs and NLP, so that LRP can be used in most settings in materials and chemistry {cite}`Montavon2019`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shapley Values\n",
    "\n",
    "A model agnostic way to treat feature importance is with **Shapley values.** Shapley values come from game theory and are a solution to how to pay a coalition of cooperating players according to their contributions. Imagine each feature is a player and we would like to \"pay\" them according to their contribution to the predicted value. A Shapley value $\\phi_i$ is the pay to feature $i$. We break-up the predicted function value $\\hat{f}(x)$ into the Shapley values so that the sum of the pay is the function value ($\\sum_i \\phi_i = \\hat{f}(x)$. This means you can interpet the Shapley value of a feature as its numerical contribution to the prediction. Shapley values are powerful because their calculation is agnostic to the model, they partition the predicted value among each feature, and they have other attributes that we would desire in an explanation of a prediction (symmetry, linearity, permutation invariant, etc.). Their disadvantage are that exact computation is combinatorial with respect to feature number and they have no sparsity, making them less helpful as feature number grows. Most methods we discuss here also have no sparsity -- you can also force your model to be sparse to achieve sparse explanations.\n",
    "\n",
    "Shapley values are computed as\n",
    "\n",
    "\\begin{equation}\n",
    "\\phi_i = \\frac{1}{Z}\\sum_{S \\in M \\backslash x_i}v(S\\cup x_i) - v(S)\n",
    "\\end{equation}\n",
    "$$\n",
    "Z = \\frac{|S|!\\left(N - |S| - 1\\right)!}{N!}\n",
    "$$\n",
    "\n",
    "where $S \\in N \\backslash x_i$ means all sets of features that exclude feature $x_i$, $S\\cup x_i$ means putting back feature $x_i$ into the set, and $v(S)$ is the value of $\\hat{f}(x)$ using only the features included in $S$, and $Z$ is a normalization value. The formula can be interpreted as the mean of all possible differences in $\\hat{f}$ formed by adding/removing feature $i$. \n",
    "\n",
    "One immediate concern though is how can we \"remove\" feature $i$ from a model equation? We marginilize out feature $i$. Recall a marginal is a way to integrate out a random variable -- $P(x) = \\int\\, P(x,y)\\,dy$. That integrates over all possible $x$ values. Marginalization can be used on functions of random variables, which obviously are also random variables, by taking an expectation: $E_y[f | X = x] = \\int\\,f(X=x,y)P(X=x,y)\\, dy$. I've emphasized that the random variable $X$ is fixed in the integral and thus $E_y[f]$ is a function of $x$. $y$ is removed by computing the expected value of $f(x,y)$ where $x$ is fixed (the function argument). We're essentially replacing $f(x,y)$ with a new function $E_y[f]$ that is the average of all possible $y$ values I'm over-explaining this though, it's quite intuitive. The other detail is that *value* is the change relative to the average of $\\hat{f}$. You can typically ignore this extra term - it cancels, but I include it for completeness. Thus the value equation becomes {cite}`vstrumbelj2014explaining`:\n",
    "\n",
    "\\begin{equation}\n",
    "v(x_i) = \\int\\,f(x_0, x_1, \\ldots, x_i,\\ldots, x_N)P(x_0, x_1, \\ldots, x_i,\\ldots, x_N)\\, dx_i - E\\left[\\hat{f}(\\vec{x})\\right]\n",
    "\\end{equation}\n",
    "\n",
    "How do we compute the marginal $\\int\\,f(x_0, x_1, \\ldots, x_i,\\ldots, x_N)P(x_0, x_1, \\ldots, x_i,\\ldots, x_N)\\, dx_i$? We do not have a known probability distribution $P(\\vec{x})$. We can sample from $P(\\vec{x})$ by considering our data as an **empirical distribution**. That is, we can sample from $P(\\vec{x})$ by sampling data points. There is a little bit of complexity here because we need to sample the $\\vec{x}$'s jointly, we cannot just mix together individual features randomly because there are correlations between features that will be removed. {cite}`vstrumbelj2014explaining` showed that we can directly estimate the $i$th Shapley value with:\n",
    "\n",
    "\\begin{equation}\n",
    "\\phi_i(\\vec{x}) = \\frac{1}{M}\\sum^M \\hat{f}\\left(\\vec{z}_{+i}\\right) - \\hat{f}\\left(\\vec{z}_{+i}\\right)\n",
    "\\end{equation}\n",
    "\n",
    "where $\\vec{z}$ is a \"chimera\" exapmle constructed from the real example $\\vec{x}$ and a randomly drawn example $\\vec{x}'$. We randomly select from $\\vec{x}$ and $\\vec{x}'$ to construct $\\vec{z}$, except $\\vec{z}_{+i}$ specifically has the $i$th feature from the example $\\vec{x}$ and $\\vec{z}_{-i}$ has the $i$th feature from the random example $\\vec{x}'$. $M$ is chosen large enough to get a good sample for this value. {cite}`vstrumbelj2014explaining` gives guidance on choosing $M$, but basically as large $M$ as computationally feasible reasonable. \n",
    "\n",
    "\n",
    "With this efficient approximation method, the strong theory, and independence of model choice, Shapley values are an excellent choice for describing feature importance for examples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Importance Example\n",
    "\n",
    "Let's see an example of these two methods on the solubility prediction example from {doc}`layers`.  As a reminder, the model takes in peptides sequences and predicts solubility. The goal of the feature importance method here will be to identify which amino acids matter most for the solubility prediction. The hidden-cell below loads and reshapes the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running This Notebook\n",
    "\n",
    "\n",
    "Click the &nbsp;<i aria-label=\"Launch interactive content\" class=\"fas fa-rocket\"></i>&nbsp; above to launch this page as an interactive Google Colab. See details below on installing packages, either on your own environment or on Google Colab\n",
    "\n",
    "````{tip} My title\n",
    ":class: dropdown\n",
    "To install packages, execute this code in a new cell\n",
    "\n",
    "```\n",
    "!pip install matplotlib numpy pandas seaborn jax jaxlib dm-haiku\n",
    "```\n",
    "\n",
    "````"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true,
    "tags": [
     "hidden-cell"
    ]
   },
   "outputs": [],
   "source": [
    "import haiku as hk\n",
    "import jax\n",
    "import tensorflow as tf\n",
    "import jax.numpy as jnp\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import matplotlib as mpl\n",
    "import warnings\n",
    "import urllib\n",
    "import jax.experimental.optimizers as opt\n",
    "warnings.filterwarnings('ignore')\n",
    "sns.set_context('notebook')\n",
    "sns.set_style('dark',  {'xtick.bottom':True, 'ytick.left':True, 'xtick.color': '#666666', 'ytick.color': '#666666',\n",
    "                        'axes.edgecolor': '#666666', 'axes.linewidth':     0.8 , 'figure.dpi': 300})\n",
    "color_cycle = ['#1BBC9B', '#F06060', '#5C4B51', '#F3B562', '#6e5687']\n",
    "mpl.rcParams['axes.prop_cycle'] = mpl.cycler(color=color_cycle) \n",
    "\n",
    "ALPHABET = ['', 'A','R','N','D','C','Q','E','G','H','I', 'L','K','M','F','P','S','T','W','Y','V']\n",
    "def seq2array(seq):\n",
    "    N = pos_data.shape[-1]\n",
    "    return np.pad(list(map(ALPHABET.index, seq)), (0,N - len(seq))).reshape(1, -1)\n",
    "def array2oh(a):\n",
    "    a = np.squeeze(a)\n",
    "    o = np.zeros((len(a), 21))\n",
    "    o[np.arange(len(a)), a] = 1\n",
    "    return o.astype(np.float32).reshape(1, -1, 21)\n",
    "\n",
    "urllib.request.urlretrieve(\n",
    "    'https://github.com/whitead/dmol-book/raw/master/data/solubility.npz', 'solubility.npz')\n",
    "with np.load('solubility.npz') as r:\n",
    "    pos_data, neg_data = r['positives'], r['negatives']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": [
     "hidden-cell"
    ]
   },
   "outputs": [],
   "source": [
    "# create labels and stich it all into one \n",
    "# tensor\n",
    "labels = np.concatenate((np.ones((pos_data.shape[0], 1), dtype=pos_data.dtype), np.zeros((neg_data.shape[0], 1) , dtype=pos_data.dtype)), axis=0)\n",
    "features = np.concatenate((pos_data, neg_data), axis=0)\n",
    "# we now need to shuffle before creating TF dataset\n",
    "# so that our train/test/val splits are random\n",
    "i = np.arange(len(labels))\n",
    "np.random.seed(0)\n",
    "np.random.shuffle(i)\n",
    "labels = labels[i]\n",
    "features = features[i]\n",
    "L = pos_data.shape[-1]\n",
    "# convert to one-hot\n",
    "ohfeatures = np.zeros((len(labels),L,21))\n",
    "for i in range(len(labels)):\n",
    "    ohfeatures[i,np.arange(L),features[i]] = 1\n",
    "full_data = tf.data.Dataset.from_tensor_slices((ohfeatures.astype(np.float32), labels))\n",
    "\n",
    "# now split into val, test, train\n",
    "N = pos_data.shape[0] + neg_data.shape[0]\n",
    "split = int(0.1 * N)\n",
    "test_data = full_data.take(split).batch(16)\n",
    "nontest = full_data.skip(split)\n",
    "val_data, train_data = nontest.take(split).batch(16), nontest.skip(split).shuffle(1000).batch(16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We rebuild the solubility model in JAX (using Haiku) to make working with gradients a bit easier. We also make a few changes to the model -- we pass in the sequence length and amino acid fractions as extra information in addition to the convolutions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_cross_entropy(logits, y):\n",
    "    yhat = jax.nn.sigmoid(logits)\n",
    "    return -jnp.sum(y * jnp.log(yhat) + (1 - y) * jnp.log(1 - yhat + 1e-6))\n",
    "def model_fn(x):\n",
    "    # get fractions, excluding skip character\n",
    "    aa_fracs = jnp.mean(x, axis=1)[:,1:]\n",
    "    # compute convolutions/poolings\n",
    "    mask = jnp.sum(x[...,1:], axis=-1, keepdims=True)    \n",
    "    for kernel, pool in zip([5, 3, 3], [4, 2, 2]):\n",
    "        x = hk.Conv1D(16, kernel)(x) * mask\n",
    "        x = jax.nn.tanh(x)\n",
    "        x = hk.MaxPool(pool, pool, 'VALID')(x)\n",
    "        mask = hk.MaxPool(pool, pool, 'VALID')(mask)\n",
    "    # combine fractions, length, and convolution ouputs\n",
    "    x = jnp.concatenate((hk.Flatten()(x), aa_fracs, jnp.sum(x[:,1:], axis=(1,2)).reshape(-1,1)),axis=1)\n",
    "    # dense layers\n",
    "    logits = hk.Sequential([               \n",
    "        hk.Linear(256, with_bias=False), jax.nn.tanh,\n",
    "        hk.Linear(64, with_bias=False), jax.nn.tanh,\n",
    "        hk.Linear(1, with_bias=False)\n",
    "    ])(x)\n",
    "    return logits\n",
    "model = hk.without_apply_rng(hk.transform(model_fn))\n",
    "def loss_fn(params, x, y):    \n",
    "    logits = model.apply(params, x)\n",
    "    return jnp.mean(binary_cross_entropy(logits, y))\n",
    "@jax.jit\n",
    "def solubility_prob(params, x):\n",
    "    logits = model.apply(params, x)\n",
    "    return jax.nn.sigmoid(jnp.squeeze(logits))\n",
    "@jax.jit\n",
    "def accuracy_fn(params, x, y):\n",
    "    logits = model.apply(params, x)\n",
    "    return jnp.mean((logits >= 0) * y + (logits < 0) * (1 - y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = jax.random.PRNGKey(0)\n",
    "xi, yi = ohfeatures[:16], labels[:16]\n",
    "params = model.init(rng, xi)\n",
    "\n",
    "opt_init, opt_update, get_params = opt.adam(1e-3)\n",
    "opt_state = opt_init(params)\n",
    "\n",
    "@jax.jit\n",
    "def update(step, opt_state, x, y):\n",
    "    value, grads = jax.value_and_grad(loss_fn)(get_params(opt_state), x, y)\n",
    "    opt_state = opt_update(step, grads, opt_state)\n",
    "    return value, opt_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 32\n",
    "for e in range(epochs):\n",
    "    for i, (xi, yi) in enumerate(train_data):\n",
    "        value, opt_state = update(i, opt_state, xi.numpy(), yi.numpy())        \n",
    "opt_params = get_params(opt_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we compute the accuracy of our model. Remember, this is a challenging problem and so about 60--70\\% is typical on this dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.46950433\n"
     ]
    }
   ],
   "source": [
    "acc = []\n",
    "for xi, yi in test_data:\n",
    "    acc.append(accuracy_fn(opt_params, xi.numpy(), yi.numpy()))\n",
    "print(jnp.mean(np.array(acc)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try an amino acid sequence, a peptide, to get a feel for the model. The model outputs logits (logarithm of odds), which we put through a sigmoid to get probabilities. The peptides must be converted from a sequence to a matrix of one-hot column vectors. We'll try two sequences: multiple serines which should be soluble and multiple phenylalanines, which should be insoluble."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probability SSSSS of being soluble 0.63\n",
      "Probability FFFFF of being soluble 0.63\n"
     ]
    }
   ],
   "source": [
    "s = 'SSSSS'\n",
    "sm = array2oh(seq2array(s))\n",
    "sol_prob = solubility_prob(opt_params, sm)\n",
    "print(f'Probability {s} of being soluble {sol_prob:.2f}')\n",
    "\n",
    "s = 'FFFFF'\n",
    "sm = array2oh(seq2array(s))\n",
    "sol_prob = solubility_prob(opt_params, sm)\n",
    "print(f'Probability {s} of being soluble {sol_prob:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks reasonable -- the hydrophilic are likely soluble and the hydrophobic is likely insoluble. \n",
    "\n",
    "### Gradients\n",
    "\n",
    "Now to start examining *why* a particular sequence is soluble or insoluble! We'll begin by computing the gradients with respect to input -- the naieve approach that is susceptible to shattered gradients. Computing this is a component in the process for integrated and smooth gradients, so not wasted effort. We will use a more complex peptide sequence to get more interesting analysis. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "def plot_grad(g, s, ax=None):\n",
    "    #g = np.array(g)\n",
    "    if ax is None:\n",
    "        plt.figure()\n",
    "        ax = plt.gca()\n",
    "    h = g[0, np.arange(len(s)), list(map(ALPHABET.index,s))]\n",
    "    ax.bar(np.arange(len(s)), height=h)\n",
    "    ax.set_xticks(range(len(s)))\n",
    "    ax.set_xticklabels(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probability EKAALSLVVVFFRPVC of being soluble 0.63\n"
     ]
    }
   ],
   "source": [
    "s = 'EKAALSLVVVFFRPVC'\n",
    "sm = array2oh(seq2array(s))\n",
    "sol_prob = solubility_prob(opt_params, sm)\n",
    "print(f'Probability {s} of being soluble {sol_prob:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "g = jax.grad(solubility_prob,1)(opt_params, sm)\n",
    "plot_grad(g, s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember that the model outputs logits. Positive value of the gradient mean this amino acid is responsible for pushing solubility probability higher and negative values mean the amino acid is pushing towards insoluble. As you might expect, hydrophilic amino acids like S and R push the solubility probability higher and valine pushes it \n",
    "down. However, some strangeness appears like the valines in the VVV part have wildly different effects.\n",
    "\n",
    "### Integrated Gradients\n",
    "\n",
    "We'll now implement the integrated gradients method. We go through three basic steps:\n",
    "\n",
    "1. Create an array of inputs going from baseline to input peptide\n",
    "2. Evaluate gradient on each input\n",
    "3. Compute the sum of the gradients and multiply it by difference between baseline and peptide\n",
    "\n",
    "The baseline for us is all zeros -- which gives a probability of solubility of 0.5 (logits = 0, a model root). This basline is exactly on the decision bounary. You could use other baselines like all glycines or all alanines, just they should be at or very near solubility of 0.5. You can find a detailed and interactive exploration of the baseline choice in {cite}`sturmfels2020visualizing`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def integrated_gradients(sm, N):\n",
    "    baseline = jnp.zeros((1, L, 21))\n",
    "    t = jnp.linspace(0, 1, N).reshape(-1, 1, 1)\n",
    "    path = baseline * (1 - t) + sm * t\n",
    "    def get_grad(pi):\n",
    "        # compute gradient \n",
    "        # add/remove batch axes\n",
    "        return jax.grad(solubility_prob, 1)(opt_params, pi[jnp.newaxis, ...])[0]\n",
    "    gs = jax.vmap(get_grad)(path)\n",
    "    # sum pieces (Riemann sum), multiply by (x - x')\n",
    "    ig = jnp.mean(gs, axis=0, keepdims=True) * (sm - baseline)\n",
    "    return ig\n",
    "ig = integrated_gradients(sm, 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_grad(ig, s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the valines are a little closer. Perhaps not that much more informative than the vanilla gradients.\n",
    "\n",
    "### SmoothGrad\n",
    "\n",
    "To do SmoothGrad, our steps are almost identicial:\n",
    "\n",
    "1. Create an array of inputs that are random pertubations of the input peptide\n",
    "2. Evaluate gradient on each input\n",
    "3. Compute the mean of the gradients\n",
    "\n",
    "There is one additional hyperparameter, $\\sigma$, which in principle should be as small as possible while still causing the model output to change.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def smooth_gradients(sm, N, rng, sigma=1e-3):\n",
    "    baseline = jnp.zeros((1, L, 21))\n",
    "    t = jax.random.normal(rng, shape=(N, sm.shape[1], sm.shape[2])) * sigma\n",
    "    path = sm + t\n",
    "    # remove examples that are negative and force summing to 1\n",
    "    path = jnp.clip(path, 0, 1)\n",
    "    path /= jnp.sum(path, axis=2, keepdims=True)    \n",
    "    def get_grad(pi):\n",
    "        # compute gradient \n",
    "        # add/remove batch axes\n",
    "        return jax.grad(solubility_prob, 1)(opt_params, pi[jnp.newaxis, ...])[0]\n",
    "    gs = jax.vmap(get_grad)(path)\n",
    "    # mean\n",
    "    ig = jnp.mean(gs, axis=0, keepdims=True)\n",
    "    return ig\n",
    "sg = smooth_gradients(sm, 256, jax.random.PRNGKey(0))\n",
    "plot_grad(sg, s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks remarkably similar to the vanilla gradient setting -- probably because our 1D input/shallow network is not as sensitive to shattered gradients."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shapley Value\n",
    "\n",
    "Now we will approximate the Shapley values for each feature using Equation XX.The Shapley value computation is different than previous approaches because it does not require gradients. The basic algorithm is:\n",
    "\n",
    "1. select random point x'\n",
    "2. create point z by combining x and x'\n",
    "3. compute change in predicted function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeviceArray([-5.5642126e-06, -5.5642126e-06, -5.5642126e-06,\n",
       "             -5.5642126e-06, -5.5642126e-06, -5.5642126e-06,\n",
       "             -5.5642126e-06, -5.5642126e-06, -5.5642126e-06,\n",
       "             -5.5642126e-06], dtype=float32)"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def shapley(i, sm, sampled_x, rng, model, opt_params=opt_params):\n",
    "    M = sampled_x.shape[0]\n",
    "    z_choice = jax.random.bernoulli(rng, shape=(M,))\n",
    "    # construct with and w/o ith feature\n",
    "    z_choice = jax.ops.index_update(z_choice, i, 0.)\n",
    "    z_choice_i = jax.ops.index_update(z_choice, i, 1.0)\n",
    "    # select them via multipleication\n",
    "    z = sm[jnp.newaxis] * z_choice[:, jnp.newaxis, jnp.newaxis] + \\\n",
    "        sampled_x * (1 - z_choice[:, jnp.newaxis, jnp.newaxis])\n",
    "    z_i = sm[jnp.newaxis] * z_choice_i[:, jnp.newaxis, jnp.newaxis] + \\\n",
    "        sampled_x * (1 - z_choice_i[:, jnp.newaxis, jnp.newaxis])\n",
    "    v = model(opt_params, jnp.squeeze(z_i)) - model(opt_params, jnp.squeeze(z))\n",
    "    return jnp.mean(v, axis=0)\n",
    "    \n",
    "# assume data is alrady shuffled, so just take M\n",
    "M = 500\n",
    "sampled_x = train_data.unbatch().batch(M).as_numpy_iterator().next()[0]\n",
    "# make batched shapley so we can compute for all features\n",
    "bshapley = jax.vmap(shapley, in_axes=(0, None, None, 0, None))\n",
    "bshapley(jnp.arange(10), sm, sampled_x, jax.random.split(jax.random.PRNGKey(0), 10), solubility_prob)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The three methods are shown side-by-side below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "heights = []\n",
    "plt.figure(figsize=(12,4))\n",
    "x = np.arange(len(s))\n",
    "for i,(gi,l) in enumerate(zip([g, ig, sg], ['Gradient', 'Integrated', 'Smooth'])):\n",
    "    h = gi[0, np.arange(len(s)), list(map(ALPHABET.index,s))]\n",
    "    plt.bar(x + i/4 - 1/4, h, width=1/4, edgecolor='black', label=l)\n",
    "plt.gca().set_xticks(range(len(s)))\n",
    "plt.gca().set_xticklabels(s)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What can we conclude from this information? We could perhaps add an explanation like this: \"The sequence is predicted to be soluble primarily because of the serine.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shapley Values\n",
    "\n",
    "A model agnostic way to treat feature importance is with **Shapley values.** Shapley values come from game theory and are a solution to how to pay a coalition of cooperating players according to their contributions. Imagine each feature is a player and we would like to \"pay\" them according to their contribution to the predicted value. A Shapley value $\\phi_i$ is the pay to feature $i$. We break-up the predicted function value $\\hat{f}(x)$ into the Shapley values so that the sum of the pay is the function value ($\\sum_i \\phi_i = \\hat{f}(x)$. This means you can interpet the Shapley value of a feature as its contribution to the prediction. Shapley values are powerful because their calculation is agnostic to the model, they equally partition the predicted value to each feature, and they have other attributes that we would desire in an explanation of a prediction (symmetry, linearity, permutation invariant, etc.). Their disadvantages are that exact computation is combinatorial with respect to feature number and they have no sparsity, making them less helpful as feature number grows. Although most methods we discuss here also have no sparsity -- you can also force your model to be sparse to achieve sparse explanations.\n",
    "\n",
    "Shapley values are computed as\n",
    "\n",
    "\\begin{equation}\n",
    "\\phi_i = \\frac{1}{Z}\\sum_{S \\in M \\backslash x_i}v(S\\cup x_i) - v(S)\n",
    "\\end{equation}\n",
    "$$\n",
    "Z = \\frac{|S|!\\left(N - |S| - 1\\right)!}{N!}\n",
    "$$\n",
    "\n",
    "where $S \\in N \\backslash x_i$ means all sets of features that exclude feature $x_i$, $S\\cup x_i$ means putting back feature $x_i$ into the set, and $v(S)$ is the value of $\\hat{f}(x)$ using only the features included in $S$, and $Z$ is a normalization value. The formula can be interpreted as the mean of all possible differences in $\\hat{f}$ formed by adding/removing feature $i$. \n",
    "\n",
    "One immediate concern though is how we can \"remove\" feature $i$ from a model equation? We marginilize out feature $i$. Recall a marginal is a way to integrate out a random variable -- $P(x) = \\int\\, P(x,y)\\,dy$. That integrates over all possible $x$ values. Marginalization can be used on functions of random variables, which obviously are also random variables, by taking an expectation: $E_y[f | X = x] = \\int\\,f(X=x,y)P(X=x,y)\\, dy$. I've emphasized that the random variable $X$ is fixed in the integral and thus $E_y[f]$ is a function of $x$. $y$ is removed by computing the expected value of $f(x,y)$ where $x$ is fixed (the function argument). We're essentially replacing $f(x,y)$ with a new function $E_y[f]$ that is the average of all possible $y$ values I'm over-explaining this though, it's quite intuitive. The other detail is that *value* is the change relative to the average of $\\hat{f}$. You can typically ignore this extra term - it cancels, but I include it for completeness. Thus the value equation becomes {cite}`vstrumbelj2014explaining`:\n",
    "\n",
    "\\begin{equation}\n",
    "v(x_i) = \\int\\,f(x_0, x_1, \\ldots, x_i,\\ldots, x_N)P(x_0, x_1, \\ldots, x_i,\\ldots, x_N)\\, dx_i - E\\left[\\hat{f}(\\vec{x})\\right]\n",
    "\\end{equation}\n",
    "\n",
    "How do we compute the marginal $\\int\\,f(x_0, x_1, \\ldots, x_i,\\ldots, x_N)P(x_0, x_1, \\ldots, x_i,\\ldots, x_N)\\, dx_i$? We do not have a known probability distribution $P(\\vec{x})$. We can sample from $P(\\vec{x})$ by considering our data as an **empirical distribution**. That is, we can sample from $P(\\vec{x})$ by sampling data points. There is a little bit of complexity here because we need to sample the $\\vec{x}$'s jointly, we cannot just mix together individual features randomly because there are correlations between features that will be removed. {cite}`vstrumbelj2014explaining` showed that we can directly estimate the $i$th Shapley value with:\n",
    "\n",
    "\\begin{equation}\n",
    "\\phi_i(\\vec{x}) = \\frac{1}{M}\\sum^M \\hat{f}\\left(\\vec{z}_{+i}\\right) - \\hat{f}\\left(\\vec{z}_{+i}\\right)\n",
    "\\end{equation}\n",
    "\n",
    "where $\\vec{z}$ is a \"chimera\" exapmle constructed from the real example $\\vec{x}$ and a randomly drawn example $\\vec{x}'$. We randomly select from $\\vec{x}$ and $\\vec{x}'$ to construct $\\vec{z}$, except $\\vec{z}_{+i}$ specifically has the $i$th feature from the example $\\vec{x}$ and $\\vec{z}_{-i}$ has the $i$th feature from the random example $\\vec{x}'$. $M$ is chosen large enough to get a good sample for this value. {cite}`vstrumbelj2014explaining` gives guidance on choosing $M$, but basically as large $M$ as computationally feasible reasonable. \n",
    "\n",
    "\n",
    "With this approximation method, the strong theory, and independence of model choice, Shapley values are an excellent choice for describing feature importance for examples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Data Importance\n",
    "\n",
    "Another kind of explanation or interpretation we might desire is *which* training data points contribute most to a prediction. This is a more literal answer to the question: \"Why did my model predict this?\" -- neural networks are a result of training data and thus the answer to why a prediction is made can be traced to training data. Ranking training data for a given prediction helps us understand which training examples are causing the neural network to predict a value. This is like an influence function, $\\mathcal{I}(x_i, x)$, which gives a score of incluence for training point $i$ and input $x$. The most straightforward way to compute the influence would be to train the neural network with ($\\hat{f}(x)$) and without $x_i$ ($\\hat{f}_{-x_i}(x)$) and define the influence as\n",
    "\n",
    "\\begin{equation}\n",
    "\\mathcal{I}(x_i, x) = \\hat{f}_{-x_i}(x) - \\hat{f}(x)\n",
    "\\end{equation}\n",
    "\n",
    "For example, if a prediction is higher after removing the training point $x_i$ from training, we would say that point has a positive influence. Computing this influence function requires training the model as many times as you have points -- typically computationally infeasible. {cite}`koh2017understanding` show a way to approximate this by looking at infinitesimal changes to the *weights* of each training point. Computing these influence functions do require computing a Hessian with respect to the loss function and thus are not commonly used. If you're using JAX though, this is simple to do.\n",
    "\n",
    "```{margin}\n",
    "If using a kernel model, remembering that the features are the training data. Then the above methods, like integrated gradients, give training data importance.\n",
    "```\n",
    "\n",
    "Training data importance provides an interpretation that is useful for deep learning experts. It tells you which training examples are most influential for a given predictions This can help troubleshoot issues with data or tracing explanations for spurious predictions. However, a typical user of predictions from a deep learning model will probably be unsatisfied with a ranking of training data as an explanation. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cited References\n",
    "\n",
    "```{bibliography}\n",
    ":style: unsrtalpha\n",
    ":filter: docname in docnames\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
