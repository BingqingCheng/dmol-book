{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Input Data & Equivariances\n",
    "\n",
    "Molecular graphs and structures (xyz coordinates) are the fundamental features in molecules and materials. As discussed, often these are converted into *molecular descriptors* or some other representation. Why is that? Why can we not work with the data directly? For example, let's say we have a butane molecule and would like to predict its potential energy from its position. You could train a linear model $\\hat{E}$ that predicts energy\n",
    "\n",
    "\\begin{equation}\n",
    "    \\hat{E} = \\textbf{X}\\textbf{W} + b\n",
    "\\end{equation}\n",
    "\n",
    "where $\\textbf{X}$ is $14$ (atoms) $\\times$ $3$ (xyz coordinates) matrix containing positions and $\\textbf{W}, b$ are trainable parameters. So far, this is all reasonable. Now what if I translate all the coordinates by -10:\n",
    "\n",
    "\\begin{equation}\n",
    "\\left(\\textbf{X} - 10\\right)\\textbf{W} + b = \\textbf{X} + b - 10 |\\textbf{W}|\n",
    "\\end{equation}\n",
    "\n",
    "We know the energy should not change if we translate all the coordinates equally -- the molecule is not changing conformations. However, our linear regression will change by $ - 10 |\\textbf{W}|$. We have accidentally made our model sensitive to the origin of our coordinate system, which is not physical. This is **translational variance** -- our model changes when we translate the coordinates.\n",
    "\n",
    "Consider another example from our butane molecule. What if we swapped the order of the atoms in our $\\textbf{X}$ matrix. There is no such thing as a \"left\" or \"right\" side of our molecule, so it should not matter. However, you'll see that changing the order of $\\textbf{X}$ changes which weights are multiplied and thus the predicted energy will change. This is called a **permutation variance**. Our model changes if we re-order or inputs, even though from our knowledge of chemistry this should not matter. Similarly, our output energy should not be sensitive to a rotation of the molecular coordinates. \n",
    "\n",
    "```{margin}\n",
    "You could teach your model to learn permutation variance of left/right in this example, either by making your training data contain multiple orderings of $\\textbf{X}$ or somehow making your $\\textbf{W}$ be symmetric. You can accomplish this via data augmentation. However, this is inefficient because the number of permutations you want to train the model to ignore is combinatorially large. \n",
    "```\n",
    "\n",
    "## Equivariances \n",
    "Models that work with molecules must be permutation equivariant. Permutation equivariant means that if you rearrange the order of atoms, the output changes in the same way. For example, if you're predicting partial charge per atom $\\vec{y} = f(\\textbf{X})$ and you rearrange $\\textbf{X}$, you expect the $\\vec{y}$ to rearrange to match that. Let's try to state this with an equation. Consider $\\mathcal{P}_{34}$ to be the *permutation operator*. It swaps indices 3 and 4 in axis 0 of a tensor. Then a permutation equivariant model equation should have:\n",
    "\n",
    "\n",
    "\\begin{equation}\n",
    "    f\\left(\\mathcal{P}_{ij}\\left[\\textbf{X}\\right]\\right) = \\mathcal{P}_{ij}\\left[\\vec{y}\\right], \\quad i,j\n",
    "\\end{equation}\n",
    "\n",
    "for any $i, j$. This is a bit of a narrow definition, you can find a more general form in {cite}`thomas2018tensor`. Now what happens if we output a scalar like energy? Then our permutation operator does nothing:\n",
    "\n",
    "\n",
    "\\begin{equation}\n",
    "    f\\left(\\mathcal{P}_{ij}\\left[\\textbf{X}\\right]\\right) = \\mathcal{P}_{ij}\\left[\\hat{E}\\right] = \\hat{E}\n",
    "\\end{equation}\n",
    "\n",
    "we call this case a **permutation invariance**.\n",
    "\n",
    "```{margin}\n",
    "The classic way to introduce equivariance is through group theory. Rather than teach group theory here, I'll use simpler equations that do not quite fully capture the ideas and power of equivariances but get the point across quickly. \n",
    "```\n",
    "\n",
    "```{note}\n",
    "An invariance is special type of equivariance. If something is equivariant, you can easily make it invariant (e.g., averaging over your equivariant axes). \n",
    "```\n",
    "\n",
    "## Equivariances of Coordinates\n",
    "\n",
    "When we work with molecular coordinates as features we need to be a bit more careful in distinguishing between the \"features\" that might be element identity and those which specify the location in space. This kind of data is referred to as **point clouds** in computer science. Let's break our features into $(\\vec{r}, \\vec{x})$ where $\\vec{r}$ is the location of the atom/point and $\\vec{x}$ are its features (e.g., element, charge, spin, etc.). Similarly, we may have labels at each point so that we write our labels as $(\\vec{r}', \\vec{y})$. The label might be direction $\\vec{r}'$ and force magnitude $y$ or perhaps a field at $\\vec{r}'$ with vectors $\\vec{y}$. You may not have $\\vec{r}'$ like if you're predicting energy. It may be that $\\vec{r} = \\vec{r}'$. With this notation, we can write out **translation equivariance** as:\n",
    "\n",
    "\\begin{equation}\n",
    "    f\\left(\\vec{r} + \\vec{t}, \\vec{x}\\right) = (\\vec{r}' + \\vec{t}, \\vec{y}), \\quad  \\forall\\vec{t}\n",
    "\\end{equation}\n",
    "\n",
    "You can also write this out if you have a matrix of atom positions (like a molecule):\n",
    "\n",
    "\\begin{equation}\n",
    "    f\\left(\\textbf{R} + \\vec{t}, \\textbf{X}\\right) = (\\textbf{R}' + \\vec{t}, \\vec{y}_i), \\quad  \\forall\\vec{t}\n",
    "\\end{equation}\n",
    "\n",
    "In the case that you do not have output coordinates $\\vec{r}'$, then it becomes:\n",
    "\n",
    "\\begin{equation}\n",
    "    f\\left(\\textbf{R} + \\vec{t}, \\textbf{X}\\right) = \\vec{y}_i, \\quad  \\forall\\vec{t}\n",
    "\\end{equation}\n",
    "\n",
    "which we call **translation invariance.** It's important to note that these equations do not apply to some specific $\\vec{t}$, but any $\\vec{t}$. \n",
    "\n",
    "\n",
    "**Rotational equivariance** can be similarly defined. Consider $\\mathcal{R}$ to be a rotation operator (e.g., a quaternion). Then our rotation equivariance equation is \n",
    "\n",
    "\\begin{equation}\n",
    "    f\\left(\\mathcal{R}\\left[\\textbf{R}\\right], \\textbf{X}\\right) = (\\mathcal{R}\\left[\\textbf{R}'\\right], \\vec{y}_i), \\quad  \\forall\\mathcal{R}\n",
    "\\end{equation}\n",
    "\n",
    "an example might be again that $(\\textbf{R}', \\vec{y}_i)$ defines some field and our equivariance says that if we rotate our input points, our output points will obey the same rotation. Again, we can also have **rotation invariance** if our model does not output $\\textbf{R}'$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constructing Equivariant Models \n",
    "\n",
    "There has been some work to unify equivariances into a single \"layer\" type so that you can just pick what equivariances you want like you would a hyperparameter {cite}`thomas2018tensor,weiler20183d`. However, these are not quite mature yet (maybe [they are?](https://github.com/e3nn/e3nn)). Instead, people usually employ a few standard \"tricks\" to get models to be equivariant. Some of the common tricks are summarized in the table below. My discussion omits [data augmentation](https://en.wikipedia.org/wiki/Data_augmentation) where you try to teach your model these equivariances through training. \n",
    "\n",
    "\n",
    "| Method | Equivariance |\n",
    "|:-------|--------:|\n",
    "| Matrix Determinant | Permutation Invariance|\n",
    "| Eigendecomposition  | Permutation Invariance |\n",
    "| Reduction (sum, mean) | Permutation Invariance|\n",
    "| Self-attention | Permutation Equivariant |\n",
    "| Graph Neural Networks | Permutation Equivariant |\n",
    "| Pairwise Distance/Vector | Translation/Rotation Invariance |\n",
    "| Convolutions | Translation Equivariance |\n",
    "| Atom-centered Symmetry Functions | Rotation/Translation Invariance |\n",
    "| Molecular Descriptors | All invariant |\n",
    "\n",
    "\n",
    "### Matrix Determinant\n",
    "\n",
    "A matrix determinant is not quite permutation invariant. If you swap two rows in a matrix, it makes the determinant change signs. However, you can easily just square the determinant to remove the sign change. How would you use a determinant in a model? It's just a building block. You could build a matrix of all neighbor features in a molecular graph and then do a determinant to arrive at a permutation invariant output. The determinant has two major disadvantages: (i) it outputs a single number and (ii) it's really expensive $O(n^3)$. Thus you won't see a determinant too frequently in deep learning.\n",
    "\n",
    "### Eigendecomposition\n",
    "\n",
    "The eigendecomposition is when you factorize a matrix into its eignvalues and eignevectors. The eignvalues of a matrix are permutation invariant. Compared with the determinant, you get more eigenvalues from a matrix than determinants so there is less loss of information. Computing eignvalues is still expensive at $O(n^3)$ and they are not differentiable. Nevertheless, you will see this strategy in kernel learning where you do not need to propogate derivatives through the kernel. One important application of this is in some of the early work on quantum machine learning {cite}`rupp2012fast`\n",
    "\n",
    "### Reductions\n",
    "\n",
    "An obvious way to remove permutation variance is to just sum over the points or atoms. More generally, you can use any kind of reduction like a mean or product. Like a determinant, this results in a single number or at least removal of an axis. Reductions are fast and differentiable. \n",
    "\n",
    "### Self-attention\n",
    "\n",
    "Attention with careful choice of keys/values and self-attention (where queries, keys, values are equal) are similar to reductions and indeed are permutation equivariant. They are not *invariant* because your query will be each point/atom and thus you should have one output for each atom/point. You must be careful though if you add weights to the self-attention, for example by multiplying the keys by a weight matrix. You can remove the equivariance by making unique weights for each point/atom index. Attention is differentiable and fast. \n",
    "\n",
    "### Graph Neural Networks\n",
    "\n",
    "Graph neural networks (GNNs) are constructed to be permutation equivariant. Also, if using the general Battaglia formulation of GNNs {cite}`battaglia2018relational` there is an additional graph level feature that is permutation invariant, making it possible to have either equivariance or invariance. \n",
    "\n",
    "\n",
    "### Pairwise Distance\n",
    "\n",
    "Using pairwise distances or vectors is the standard solution to translation invariance. Rarely are we actually that concerned with translational equivariance. If using pairwise vectors between atoms instead of the xyz coordinates, this naturally removes the choice of origin and thus makes the model translation invariant. If we go further and use pairwise distance instead of vectors, this also removes the effect of rotations giving a rotation invariance. This is fast, differentiable, and the usual approach to add translation and rotation invariance. The pairwise vectors are sometimes called **internal coordinates**. \n",
    "\n",
    "### Convolutional Layers\n",
    "\n",
    "Convolutional layers are well-known to be translationally equivariant. Remember that they work in 3D as well, so they can be an input layer for point clouds if translational equivariance is desired. \n",
    "\n",
    "### Atom-centered Symmetry Functions \n",
    "\n",
    "These are a large class of functions described by Behler{cite}`behler2011atom` that transform from the input coordinate/features $(\\mathbf{R}, \\mathbf{X})$  to a new set of features $\\mathbf{X}'$ that obey rotational and translational symmetry. This makes them translational and rotationally invariant. Behler didn't propose a single function to get these features, but instead explored the choices and theory. Bartók et al. {cite}`Bart` provided a specific recipe which is called a SOAP descriptor. These are drop-in replacements for $(\\mathbf{R}, \\mathbf{X})$ that are translation and rotation invariant but do not lose much information. They are differentiable, although complex to implement. \n",
    "\n",
    "### Molecular Descriptors\n",
    "\n",
    "Molecular descriptors are the classic way to convert molecules into translation/rotation/permutation invariant features. There exists 3D descriptors as well that can treat structure. They are also called **fingerprints**. We won't focus on these in this course because they are untrainable and choosing the correct combination of descriptors is a unsolved problem which has no clear process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examples\n",
    "\n",
    "Let's demonstrate with some code how to go about creating functions that obey these equivariances. We won't be training these models because training has no effect on equivariances\n",
    "\n",
    "```{margin}\n",
    "There are ways to make training *enforce* equivariances{cite}`ravanbakhsh2017equivariance`, but it's a somewhat complex and rarely used strategy. This is different than data augmentation, where we hope it learns these.\n",
    "```\n",
    "\n",
    "I'll define my butane molecule as a set of coordinates $\\mathbf{R}_i$ and features $\\mathbf{X}_i$. My features are just one-hot vectors indicating if a particular point is a carbon atom $[1,0]$ or a hydrogen atom $[0,1]$. In our example, we will just be interested in predicting energy. We will not train our models, so the enegry will not be accurate. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "R_i = np.array([\n",
    "      -0.5630,    0.5160,    0.0071,\n",
    "    0.5630,   -0.5159,    0.0071,\n",
    "   -1.9293,   -0.1506,   -0.0071,\n",
    "    1.9294,    0.1505,   -0.0071,\n",
    "   -0.4724,    1.1666,   -0.8706,\n",
    "   -0.4825,    1.1551,    0.8940, \n",
    "    0.4825,   -1.1551,    0.8940, \n",
    "    0.4723,   -1.1665,   -0.8706, \n",
    "   -2.0542,   -0.7710,   -0.9003, \n",
    "   -2.0651,   -0.7856,    0.8742, \n",
    "   -2.7203,    0.6060,   -0.0058,\n",
    "    2.0542,    0.7709,   -0.9003, \n",
    "    2.7202,   -0.6062,   -0.0059, \n",
    "    2.0652,    0.7854,    0.8743 \n",
    "]).reshape(-1, 3)\n",
    "N = R_i.shape[0]\n",
    "X_i = np.zeros((N, 2))\n",
    "X_i[:4, 0] = 1\n",
    "X_i[4:, 1] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### No equivariances\n",
    "\n",
    "A one-hidden layer dense neural network is an example of a model with no equivariances. To fit our data into this dense layer, we'll just stack the positions and features into a large input tensor and output energy. We'll use a tanh as activation, 16 hidden layer dimension, and our output layer has no activation because we're doing regression to energy. Our weights will always be randomly initialized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# our 1-hidden layer model\n",
    "def hidden_model(r, x, w1, w2, b1, b2):\n",
    "    # stack into one large input\n",
    "    i = np.concatenate((r, x), axis=1).flatten()\n",
    "    v = np.tanh(i @ w1 + b1)\n",
    "    v = v @ w2 + b2\n",
    "    return v\n",
    "\n",
    "# initialize our weights\n",
    "w1 = np.random.normal(size=(N * 5, 16)) # 5 -> 3 xyz + 2 features\n",
    "b1 = np.random.normal(size=(16,))\n",
    "w2 = np.random.normal(size=(16,))\n",
    "b2 = np.random.normal()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see what the predicted energy is with our coordinates. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.859279720342696"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hidden_model(R_i, X_i, w1, w2, b1, b2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is not trained, so we aren't that concerned about the value. Now let's see if our model is sensitive to translation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1.5028355476103146"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hidden_model(R_i + np.array([1, 2, 3]), X_i, w1, w2, b1, b2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, it is sensitive to translation. I added the vector $(1, 2, 3)$ to all input points and the energy changed. This model is not translation invariant. The choice of $(1,2,3)$ is arbitrary, the model should not change output regardless of the choice of the translation vector if the model is translation invariant. Rotations can be done using the `scipy.transformation` library which takes care of some of the messiness of working with quaternions, which are the operators that perform rotations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No rotation 4.859279720342696\n",
      "Rotated 1.9059462807306669\n"
     ]
    }
   ],
   "source": [
    "import scipy.spatial.transform as trans\n",
    "\n",
    "# rotate around x coord by 45 degrees\n",
    "rot = trans.Rotation .from_euler('x', 45, degrees=True)\n",
    "\n",
    "print('No rotation', hidden_model(R_i, X_i, w1, w2, b1, b2))\n",
    "print('Rotated', hidden_model(rot.apply(R_i), X_i, w1, w2, b1, b2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our model is affected by the rotation, meaning it is not rotation invariant. Permutation invariance comes from swapping indices. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original 4.859279720342696\n",
      "permuted 4.2198269367012715\n"
     ]
    }
   ],
   "source": [
    "# swap 0, 1 rows\n",
    "perm_R_i = np.copy(R_i)\n",
    "perm_R_i[0], perm_R_i[1] = R_i[1], R_i[0]\n",
    "# we do not need to swap X_i 0,1 because they are identical\n",
    "\n",
    "print('original', hidden_model(R_i, X_i, w1, w2, b1, b2))\n",
    "print('permuted', hidden_model(perm_R_i, X_i, w1, w2, b1, b2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our model is not permutation invariant!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Permutation Invariant\n",
    "\n",
    "We will use a reduction to achieve permutation invariance. All that is needed is to ensure that weights are not a function of our atom number axis and then do a reduction (sum) prior to the output layer. Here is the implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# our 1-hidden layer model with perm inv\n",
    "def hidden_model_pi(r, x, w1, w2, b1, b2):\n",
    "    # stack into one large input\n",
    "    i = np.concatenate((r, x), axis=1)\n",
    "    v = np.tanh(i @ w1 + b1)\n",
    "    # reduction\n",
    "    v = np.sum(v, axis=0)\n",
    "    v = v @ w2 + b2\n",
    "    return v\n",
    "\n",
    "# initialize our weights\n",
    "w1 = np.random.normal(size=(5, 16)) # note it no lonegr has N!\n",
    "b1 = np.random.normal(size=(16,))\n",
    "w2 = np.random.normal(size=(16,))\n",
    "b2 = np.random.normal()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We made three changes: we kept the atom axis (no more `flatten`), we removed the atom axis after the hidden layer (`sum`), and we made our weights not depend on the atom axis. Now let's observe if this is indeed permutation invariant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original -13.959628238544678\n",
      "permuted -13.959628238544678\n"
     ]
    }
   ],
   "source": [
    "print('original', hidden_model_pi(R_i, X_i, w1, w2, b1, b2))\n",
    "print('permuted', hidden_model_pi(perm_R_i, X_i, w1, w2, b1, b2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Translation Invariant\n",
    "\n",
    "The next change we will make is to convert our $N\\times3$ shaped coordinates into $N\\times N\\times 3$ pairwise distance vectors. This gives us translation invariance. This causes an issue because our distance features went from being $3$ per atom to $N \\times 3$ per atom. Thus we've introduced a dependence on atom number in our distance features and that means it's easy to accidentally break our permutation invariance. We can just sum over this new axis though. Let's see an implementation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# our 1-hidden layer model with perm inv, trans inv\n",
    "def hidden_model_pti(r, x, w1, w2, b1, b2):\n",
    "    # compute pairwise distances using broadcasting\n",
    "    d = r - r[:,np.newaxis]    \n",
    "    # stack into one large input of N x N x 5\n",
    "    # concatenate doesn't broadcast, so I manually broadcast the Nx2 x matrix\n",
    "    # into N x N x 2\n",
    "    i = np.concatenate((d, np.broadcast_to(x, (d.shape[:-1] + x.shape[-1:]))), axis=-1)    \n",
    "    v = np.tanh(i @ w1 + b1)\n",
    "    # reduction over both axes\n",
    "    v = np.sum(v, axis=(0,1))\n",
    "    v = v @ w2 + b2\n",
    "    return v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original -165.31919261250673\n",
      "permuted -165.3191926125067\n",
      "translated -165.3191926125067\n",
      "rotated -155.92637561013507\n"
     ]
    }
   ],
   "source": [
    "print('original', hidden_model_pti(R_i, X_i, w1, w2, b1, b2))\n",
    "print('permuted', hidden_model_pti(perm_R_i, X_i, w1, w2, b1, b2))\n",
    "print('translated', hidden_model_pti(R_i + np.array([-2, 3, 4]), X_i, w1, w2, b1, b2))\n",
    "print('rotated', hidden_model_pti(rot.apply(R_i), X_i, w1, w2, b1, b2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is now translation and permutation invariant. But not yet rotation invariant.\n",
    "\n",
    "### Rotation Invariant\n",
    "\n",
    "It is a simple change to make it rotationally invariant. We just convert the pairwise distance vectors into pairwise distances. We'll use squared distances for simplicity. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original -1145.2190359080248\n",
      "permuted -1145.2190359080246\n",
      "translated -1145.2190359080248\n",
      "rotated -1145.2190359080246\n"
     ]
    }
   ],
   "source": [
    "# our 1-hidden layer model with perm, trans, rot inv.\n",
    "def hidden_model_ptri(r, x, w1, w2, b1, b2):\n",
    "    # compute pairwise distances using broadcasting\n",
    "    d = r - r[:,np.newaxis]  \n",
    "    # x^2 + y^2 + z^2 of pairwise distance vectors\n",
    "    # keepdims so we get an N x N x 1 output\n",
    "    d2 = np.sum(d**2, axis=-1, keepdims=True)\n",
    "    # stack into one large input of N x N x 3\n",
    "    # concatenate doesn't broadcast, so I manually broadcast the Nx2 x matrix\n",
    "    # into N x N x 2\n",
    "    i = np.concatenate((d2, np.broadcast_to(x, (d2.shape[:-1] + x.shape[-1:]))), axis=-1)    \n",
    "    v = np.tanh(i @ w1 + b1)\n",
    "    # reduction over both axes\n",
    "    v = np.sum(v, axis=(0,1))\n",
    "    v = v @ w2 + b2\n",
    "    return v\n",
    "\n",
    "# initialize our weights\n",
    "w1 = np.random.normal(size=(3, 16)) # now just 1 dist feature\n",
    "b1 = np.random.normal(size=(16,))\n",
    "w2 = np.random.normal(size=(16,))\n",
    "b2 = np.random.normal()\n",
    "\n",
    "# test it\n",
    "\n",
    "print('original', hidden_model_ptri(R_i, X_i, w1, w2, b1, b2))\n",
    "print('permuted', hidden_model_ptri(perm_R_i, X_i, w1, w2, b1, b2))\n",
    "print('translated', hidden_model_ptri(R_i + np.array([-2, 3, 4]), X_i, w1, w2, b1, b2))\n",
    "print('rotated', hidden_model_ptri(rot.apply(R_i), X_i, w1, w2, b1, b2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have achieved our invariances! Remember that you could use different choices to achieve these invariances. Also, you may not want an invariance sometimes. You may want equivariances or are not concerned at all. For example, if you're always working with one molecule you may never need to switch around atom orders. \n",
    "\n",
    "Finally as a sanity check, let's make sure that if we change the coordinates our predicted energy changes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "changed -1187.9524630205922\n"
     ]
    }
   ],
   "source": [
    "R_i[0] = 2.\n",
    "print('changed', hidden_model_ptri(R_i, X_i, w1, w2, b1, b2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our model is still sensitive to the input features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cited References\n",
    "\n",
    "```{bibliography} references.bib\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
