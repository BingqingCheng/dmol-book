{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# QM9 Modeling\n",
    "\n",
    "QM9 is a dataset of 134,000 molecules consisting of 9 heavy atoms drawn from the elements C, H, O, N, F{cite}`ramakrishnan2014quantum`. The features are the xyz coordinates ($\\mathbf{X}$) and elements ($\\vec{e}$) of the molecule. The coordinates are determined from B3LYP/6-31G(2df,p) level DFT geometry optimization. There are multiple labels (see table below), but we'll be interested specifically in the energy of formation (Enthalpy at 298.15 K). The goal in this chapter is to see how we can model this data.\n",
    "\n",
    "\n",
    "QM9 is one of the most popular dataset for machine learning and deep learning since it came out in 2014. The first papers could achieve about 10 kcal/mol on this regression problem and now are down to ~1 kcal/mol and lower. Any model on this dataset must be translation, rotation, and permutation invariant. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Label Description\n",
    "\n",
    "|Index | Name | Units | Description|\n",
    " |:-----|-------|-------|-----------:|\n",
    "  |0  |index  |   -            |Consecutive, 1-based integer identifier of molecule|\n",
    "  |1  |A      |   GHz          |Rotational constant A|\n",
    "  |2  |B      |   GHz          |Rotational constant B|\n",
    "  |3  |C      |   GHz          |Rotational constant C|\n",
    "  |4  |mu     |   Debye        |Dipole moment|\n",
    "  |5  |alpha  |   Bohr^3       |Isotropic polarizability|\n",
    "  |6  |homo   |   Hartree      |Energy of Highest occupied molecular orbital (HOMO)|\n",
    "  |7  |lumo   |   Hartree      |Energy of Lowest occupied molecular orbital (LUMO)|\n",
    " |8 | gap   |    Hartree     | Gap, difference between LUMO and HOMO|\n",
    " |9 | r2    |    Bohr^2      | Electronic spatial extent|\n",
    " |10 | zpve  |    Hartree     | Zero point vibrational energy|\n",
    " |11 | U0    |    Hartree     | Internal energy at 0 K|\n",
    " |12 | U     |    Hartree     | Internal energy at 298.15 K|\n",
    " |13 | H     |    Hartree     | Enthalpy at 298.15 K|\n",
    " |14 | G     |    Hartree     | Free energy at 298.15 K|\n",
    " |15 | Cv    |    cal/(mol K) | Heat capacity at 298.15 K|\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n",
    "\n",
    "I have written some helper code in the `fetch_qm9.py` file. It downloads the data and converts into a format easily used in Python. The data returned from this function is broken into the features $\\mathbf{X}$ and $\\vec{e}$. $\\mathbf{X}$ is an $N\\times4$ matrix of atom positions + partial charge of the atom. $\\vec{e}$ is vector of atomic numbers for each atom in the molecule. Remember to slice the specific label you want from the label vector."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running This Notebook\n",
    "\n",
    "\n",
    "Click the &nbsp;<i aria-label=\"Launch interactive content\" class=\"fas fa-rocket\"></i>&nbsp; above to launch this page as an interactive Google Colab. See details below on installing packages, either on your own environment or on Google Colab\n",
    "\n",
    "````{tip} My title\n",
    ":class: dropdown\n",
    "To install packages, execute this code in a new cell\n",
    "\n",
    "```\n",
    "!pip install matplotlib numpy pandas seaborn tensorflow jax jaxlib\n",
    "```\n",
    "\n",
    "````"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "np.random.seed(0)\n",
    "from fetch_qm9 import fetch_qm9, get_qm9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's load the data. This step will take a few minutes as it is downloaded and processed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": [
     "remove-output"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing record file, delete if you want to re-fetch\n"
     ]
    }
   ],
   "source": [
    "qm9_records = fetch_qm9()\n",
    "data = get_qm9(qm9_records)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`data` is an iterable containing the data for the 133k molecules. Let's examine the first one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "((<tf.Tensor: shape=(5,), dtype=int64, numpy=array([6, 1, 1, 1, 1])>, <tf.Tensor: shape=(5, 4), dtype=float32, numpy=\n",
      "array([[-1.2698136e-02,  1.0858041e+00,  8.0009960e-03, -5.3568900e-01],\n",
      "       [ 2.1504159e-03, -6.0313176e-03,  1.9761203e-03,  1.3392100e-01],\n",
      "       [ 1.0117308e+00,  1.4637512e+00,  2.7657481e-04,  1.3392200e-01],\n",
      "       [-5.4081506e-01,  1.4475266e+00, -8.7664372e-01,  1.3392299e-01],\n",
      "       [-5.2381361e-01,  1.4379326e+00,  9.0639728e-01,  1.3392299e-01]],\n",
      "      dtype=float32)>), <tf.Tensor: shape=(16,), dtype=float32, numpy=\n",
      "array([ 1.0000000e+00,  1.5771181e+02,  1.5770998e+02,  1.5770699e+02,\n",
      "        0.0000000e+00,  1.3210000e+01, -3.8769999e-01,  1.1710000e-01,\n",
      "        5.0480002e-01,  3.5364101e+01,  4.4748999e-02, -4.0478931e+01,\n",
      "       -4.0476063e+01, -4.0475117e+01, -4.0498596e+01,  6.4689999e+00],\n",
      "      dtype=float32)>)\n"
     ]
    }
   ],
   "source": [
    "for d in data:\n",
    "    print(d)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are Tensorflow Tensors. They can be converted to numpy arrays via `x.numpy()`. The first item is the element vector `6,1,1,1,1`. Do you recognize the elements? It's C, H, H, H, H. The positions come next. Note that the there is an extra column containing the atom partial charges, which we will not use as a feature. Finally, the last tensor is the label vector. \n",
    "\n",
    "Now we will do some processing of the data to get into a more usable format. Let's convert to numpy arrays, remove the partial charges, and convert the elements into one-hot vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Element one hots\n",
      " [[0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "Coordinates\n",
      " [[-1.2698136e-02  1.0858041e+00  8.0009960e-03]\n",
      " [ 2.1504159e-03 -6.0313176e-03  1.9761203e-03]\n",
      " [ 1.0117308e+00  1.4637512e+00  2.7657481e-04]\n",
      " [-5.4081506e-01  1.4475266e+00 -8.7664372e-01]\n",
      " [-5.2381361e-01  1.4379326e+00  9.0639728e-01]]\n",
      "Label: -40.475117\n"
     ]
    }
   ],
   "source": [
    "def convert_record(d):\n",
    "    # break up record\n",
    "    (e, x), y = d\n",
    "    # \n",
    "    e = e.numpy()\n",
    "    x = x.numpy()\n",
    "    r = x[:, :3]    \n",
    "    # use nearest power of 2 (16)\n",
    "    ohc = np.zeros((len(e), 16))\n",
    "    ohc[np.arange(len(e)), e - 1] = 1    \n",
    "    return (ohc, r), y.numpy()[13]\n",
    "\n",
    "for d in data:\n",
    "    (e,x), y = convert_record(d)\n",
    "    print('Element one hots\\n', e)\n",
    "    print('Coordinates\\n', x)\n",
    "    print('Label:', y)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example GNN Model\n",
    "\n",
    "We now can work with this data to build a model. Let's build a simple model that can model energy and obeys the invariances required of the problem. We will use a graph neural network (GNN) because it obeys permutation invariance. We will create a *graph* from the coordinates/element vector by joining all atoms to all other atoms and using their inverse pairwise distance as the edge weight. The choice of pairwise distance gives us translation and rotation invariance. The choice of inverse distance means that atoms which are far away naturally have low edge weights.\n",
    "\n",
    "I will now define our model using the Battaglia equations {cite}`battaglia2018relational`. As opposed to most examples we've seen in class, I will use the graph level feature vector $\\vec{u}$ which will ultimately be our estimate of energy. The edge update will only consider the sender and the edge weight with trainable parameters:\n",
    "\n",
    "\\begin{equation}\n",
    "      \\vec{e}^{'}_k = \\phi^e\\left( \\vec{e}_k, \\vec{v}_{rk}, \\vec{v}_{sk}, \\vec{u}\\right) = \\sigma\\left(\\vec{v}_{sk}\\vec{w}_ee_k\\right)\n",
    "\\end{equation}\n",
    "\n",
    "where the input edge $e_k$ will be a single number (inverse pairwise distance). We will use a sum aggregation for edges (not shown). The node update will be \n",
    "\n",
    "\\begin{equation}\n",
    "   \\vec{v}^{'}_i = \\phi^v\\left( \\bar{e}^{'}_i, \\vec{v}_i, \\vec{u}\\right) = \\sigma\\left(\\mathbf{W}_v \\bar{e}^{'}_i\\right) + \\vec{v}_i\n",
    "\\end{equation}\n",
    "\n",
    "The global node aggregation will also be a sum. Finally we have our graph feature vector update:\n",
    "\n",
    "\\begin{equation}\n",
    "    \\vec{u}^{'} = \\phi^u\\left( \\bar{e}^{'},\\bar{v}^{'}, \\vec{u}\\right) = \\sigma\\left(\\mathbf{W}_u\\bar{v}^{'}\\right) + \\vec{u}\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "To compute the final energy, we'll use our regression equation:\n",
    "\n",
    "\\begin{equation}\n",
    "    \\hat{E} = \\vec{w}\\cdot \\vec{u} + b\n",
    "\\end{equation}\n",
    "\n",
    "One final detail is that we will pass on $\\vec{u}$ and the nodes, but we will keep the edges the same at each GNN layer. Remember this is an example model: there are many changes that could be made to the above. Also, it is not kernel learning which is the favorite for this domain. Let's implement it though and see if it works. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### JAX Model Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax.numpy as jnp\n",
    "import jax.experimental.optimizers as optimizers\n",
    "import jax\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def x2e(x):\n",
    "    '''convert xyz coordinates to inverse pairwise distance'''    \n",
    "    r2 = jnp.sum((x - x[:, jnp.newaxis, :])**2, axis=-1)\n",
    "    e = jnp.where(r2 != 0, 1 / r2, 0.)\n",
    "    return e\n",
    "\n",
    "def gnn_layer(nodes, edges, features, we, wv, wu):\n",
    "    '''Implementation of the GNN'''\n",
    "    # make nodes be N x N so we can just multiply directly\n",
    "    ek = jax.nn.relu(\n",
    "        jnp.repeat(nodes[jnp.newaxis,...], nodes.shape[0], axis=0) @ we * edges[...,jnp.newaxis])\n",
    "    ebar = jnp.sum(ek, axis=1)\n",
    "    new_nodes = jax.nn.relu(ebar @ wv) + nodes\n",
    "    global_node_features = jnp.sum(new_nodes, axis=0)\n",
    "    new_features = jax.nn.relu(global_node_features  @ wu) + features    \n",
    "    return new_nodes, edges, new_features\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have implemented the code to convert coordinates into inverse pairwise distance and the GNN equations above. Let's test them out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input feautres [0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "output features [0.73143464 0.         0.63617605 1.041506   2.0111923  1.0279881\n",
      " 0.         0.7413224 ]\n"
     ]
    }
   ],
   "source": [
    "graph_feature_len = 8\n",
    "node_feature_len = 16\n",
    "msg_feature_len = 16\n",
    "\n",
    "# make our weights\n",
    "def init_weights(g, n, m):\n",
    "    we = np.random.normal(size=(n, m), scale=1e-1)\n",
    "    wv = np.random.normal(size=(m, n), scale=1e-1)\n",
    "    wu = np.random.normal(size=(n, g), scale=1e-1)\n",
    "    return we, wv, wu\n",
    "\n",
    "# make a graph\n",
    "nodes = e\n",
    "edges = x2e(x)\n",
    "features = jnp.zeros(graph_feature_len)\n",
    "\n",
    "# eval\n",
    "out = gnn_layer(nodes, edges, features, *init_weights(graph_feature_len, node_feature_len, msg_feature_len))\n",
    "print('input feautres', features)\n",
    "print('output features', out[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! Our model can update the graph features. Now we need to convert this into callable and loss. We'll stack two GNN layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get weights for both layers\n",
    "w1 = init_weights(graph_feature_len, node_feature_len, msg_feature_len)\n",
    "w2 = init_weights(graph_feature_len, node_feature_len, msg_feature_len)\n",
    "w3 = np.random.normal(size=(graph_feature_len))\n",
    "b = -325. # starting guess\n",
    "\n",
    "@jax.jit\n",
    "def model(nodes, coords, w1, w2, w3, b):\n",
    "    f0 = jnp.zeros(graph_feature_len)\n",
    "    e0 = x2e(coords)\n",
    "    n0 = nodes\n",
    "    n,e,f = gnn_layer(n0, e0, f0, *w1)\n",
    "    n,e,f = gnn_layer(n, e, f, *w2)\n",
    "    yhat = f @ w3 + b\n",
    "    return yhat\n",
    "\n",
    "def loss(nodes, coords, y, w1, w2, w3, b):\n",
    "    return (model(nodes, coords, w1, w2, w3, b) - y)**2\n",
    "loss_grad = jax.grad(loss, (3, 4, 5, 6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're going to do something a little different. Since we cannot batch our graphs (all different numbers of atoms), we'll compute a few gradients to build a mini-batch before updating the parameters. Why do we want to do this? Batches can improve our gradient updates and are not just a computational convenience."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0 loss: 58.11\n",
      "epoch: 1 loss: 59.59\n",
      "epoch: 2 loss: 60.31\n",
      "epoch: 3 loss: 61.78\n",
      "epoch: 4 loss: 62.12\n",
      "epoch: 5 loss: 62.49\n",
      "epoch: 6 loss: 62.30\n",
      "epoch: 7 loss: 61.85\n",
      "epoch: 8 loss: 61.12\n",
      "epoch: 9 loss: 61.99\n",
      "epoch: 10 loss: 60.74\n",
      "epoch: 11 loss: 60.81\n",
      "epoch: 12 loss: 61.73\n",
      "epoch: 13 loss: 60.56\n",
      "epoch: 14 loss: 60.90\n",
      "epoch: 15 loss: 60.66\n"
     ]
    }
   ],
   "source": [
    "# we'll just train on 5,000 and use 1,000 for test\n",
    "test_set = data.take(1000)\n",
    "valid_set = data.skip(1000).take(100)\n",
    "train_set = data.skip(1100).take(500).shuffle(500)\n",
    "\n",
    "epochs = 16\n",
    "batch_size = 32\n",
    "eta = 1e-2\n",
    "val_loss = [0. for _ in range(epochs)]\n",
    "for epoch in range(epochs):\n",
    "    bi = 0\n",
    "    grad_est = None\n",
    "    for d in train_set:         \n",
    "        # do training step\n",
    "        # but do not update\n",
    "        # until have enough points\n",
    "        (e,x), y = convert_record(d)\n",
    "        if grad_est is None:\n",
    "            grad_est = loss_grad(e, x, y, w1, w2, w3, b)\n",
    "        else:\n",
    "            grad_est += loss_grad(e, x, y, w1, w2, w3, b)\n",
    "        bi += 1\n",
    "        if bi == batch_size:\n",
    "            # have enough to update            \n",
    "            # update regression weights\n",
    "            w3 -= eta * grad_est[2]  / batch_size\n",
    "            b -= eta * grad_est[3]  / batch_size\n",
    "            # update GNN weights            \n",
    "            for i,w in [(0, w1), (1, w2)]:\n",
    "                for j, param in enumerate(w):\n",
    "                    param -= eta * grad_est[i][j] / batch_size\n",
    "            # reset tracking of batch index\n",
    "            bi = 0            \n",
    "            grad_est = None            \n",
    "    # compute validation loss    \n",
    "    for v in valid_set:\n",
    "        (e,x), y = convert_record(v)\n",
    "        # convert SE to RMSE\n",
    "        val_loss[epoch] += jnp.sqrt(loss(e, x, y, w1, w2, w3, b) / 1000)\n",
    "    print('epoch:', epoch, 'loss: {:.2f}'.format(val_loss[epoch]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a large dataset and we're under training, but hopefully you get the principles of this process! Finally, we'll examine our parity plot. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib as mpl\n",
    "sns.set_context('notebook')\n",
    "sns.set_style('dark',  {'xtick.bottom':True, 'ytick.left':True, 'xtick.color': '#666666', 'ytick.color': '#666666',\n",
    "                        'axes.edgecolor': '#666666', 'axes.linewidth':     0.8 , 'figure.dpi': 300})\n",
    "color_cycle = ['#1BBC9B', '#F06060', '#5C4B51', '#F3B562', '#6e5687']\n",
    "mpl.rcParams['axes.prop_cycle'] = mpl.cycler(color=color_cycle) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ys = []\n",
    "yhats = []\n",
    "for v in valid_set:\n",
    "    (e,x), y = convert_record(v)\n",
    "    ys.append(y)\n",
    "    # convert SE to RMSE\n",
    "    yhats.append(model(e, x, w1, w2, w3, b))\n",
    "\n",
    "    \n",
    "plt.plot(ys, ys, '-')\n",
    "plt.plot(ys, yhats, 'o')\n",
    "plt.xlabel('Energy')\n",
    "plt.ylabel('Predicted Energy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The clusters are molecule types/sizes. You can see we're starting to get the correct trend within the clusters, but a lot of work needs to be done to move some of them. Additional learning required!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cited References\n",
    "\n",
    "```{bibliography} references.bib\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
